# 分组训练配置 - 每组 2 easy + 2 hard，每 step 包含 math/qa/code
# 修改时间: 2024-12-02

# 实验配置
exp_name: "grouped_grpo_multitype"
output_dir: "checkpoints"
log_dir: "logs"
max_steps: 500
save_every: 50
eval_every: 999999
val_samples: 87
log_every: 5

# =====================================
# 分组训练模式配置（新）
# =====================================
use_grouped_training: true  # 启用分组训练模式
grouped_data_dir: "data/grouped"  # 分组数据目录
groups_per_domain: 1  # 每种领域每 step 采样 1 组（共 3 组，12 问题）

# 分组奖励配置
grouped_reward:
  weight_easy: 0.3         # Easy 问题权重 30%
  weight_hard: 0.7         # Hard 问题权重 70%
  diversity_threshold: 0.05  # 分数差距阈值（触发多样性加分）
  diversity_weight: 0.1      # 多样性加分权重

# =====================================
# GRPO算法配置
# =====================================
adv_estimator: "grpo"
num_return_sequences_in_group: 2   # K=2，每组生成 2 个 workflow
ppo_epochs: 1
async_generation_ratio: 0
use_kl_loss: true
kl_loss_coef: 0.005
clip_range: 0.20
gamma: 1.0
lambda_gae: 0.95

# =====================================
# Batch配置
# =====================================
rollout_batch_size: 3  # 每 step 3 组（每种类型 1 组）
prompt_max_length: 3072
response_max_length: 5120

# 模型配置
base_model: "Qwen/Qwen2.5-7B-Instruct"
model_dtype: "bfloat16"

# =====================================
# LoRA配置
# =====================================
use_lora: true
lora_rank: 64
lora_alpha: 64
lora_target_modules: "q_proj,k_proj,v_proj,o_proj"
lora_dropout: 0.05

# =====================================
# 训练参数
# =====================================
learning_rate: 2.0e-5
weight_decay: 0.01
max_grad_norm: 1.0
gradient_accumulation_steps: 4
warmup_steps: 100
bf16: true
gradient_checkpointing: true

# GPU配置
device_mapping: [0]
physical_gpus: [0]
protected_pids: []
num_gpus: 1
world_size: 1

# 数据集配置（旧接口兼容）
data_dir: "data"
train_dataset: "data/ready_to_train/train_10k_final.jsonl"
val_dataset: "data/ready_to_train/test_500_preprocessed.jsonl"
test_dataset: "data/ready_to_train/test_500_preprocessed.jsonl"

# 混合采样比例（分组模式下自动保证 1:1:1）
domain_ratios:
  math: 0.333
  code: 0.333
  qa: 0.334

# AFlow配置
aflow_config_path: "config/aflow_llm.yaml"
aflow_executor_model: "gpt-4o-mini"
aflow_operators: ["Custom", "AnswerGenerate", "Programmer", "ScEnsemble", "Test", "Review", "Revise"]
aflow_operator_descriptions_path: "config/operator.json"
execution_timeout: 600

# =====================================
# 奖励配置
# =====================================
reward_weights:
  correctness: 0.65
  efficiency: 0.15
  simplicity: 0.10
  format: 0.05
  repetition: 0.05

# 提示词优化系统
experience_buffer:
  enabled: true
  buffer_size: 100
  reward_threshold: 8.0
  persistence_dir: "data/experience_buffer"

prompt_optimizer:
  enabled: true
  few_shot_k: 3
  similarity_threshold: 0.7

operator_prompt_enhancer:
  enabled: true
  top_k_examples: 2

# 监控配置
wandb:
  enabled: true
  project: "agent-prompt"
  entity: "yao110002-sdfsdfsdfsdf-com"
  api_key: "YOUR_WANDB_API_KEY"
  run_name: null

# =====================================
# Temperature调度
# =====================================
temperature_schedule:
  enabled: true
  initial: 0.5
  final: 0.15
  warmup_steps: 150

# =====================================
# 生成配置
# =====================================
generation_config:
  temperature: 0.5
  top_p: 0.95
  top_k: 50
  max_new_tokens: 4096
  do_sample: true

# 调试选项
debug: true
verbose: true

# =====================================
# WA-GRPO配置（Workflow-Aware优势计算）
# =====================================
wa_grpo:
  alpha: 0.12
  diversity_weight: 0.35
  revise_gain_weight: 0.25
  exec_success_weight: 0.20
  efficiency_weight: 0.10
  op_variety_weight: 0.10
  min_advantage_std: 0.10
  batch_calibration: true
