# LLM Judgeå®ç°æ€»ç»“ (2025-11-19 23:17)

## å®Œæˆæƒ…å†µ

âœ… **æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ**

1. âœ… ç ”ç©¶AgentFlowçš„LLM Judgeæ–¹æ³•
2. âœ… åœ¨`reward_computer.py`ä¸­å®ç°LLM JudgeåŠŸèƒ½
3. âœ… æµ‹è¯•LLM Judgeï¼ˆ6/6æµ‹è¯•é€šè¿‡ï¼Œ100%å‡†ç¡®ç‡ï¼‰
4. âœ… åœ¨GRPOè®­ç»ƒå™¨ä¸­å¯ç”¨LLM Judge
5. âœ… é‡å¯è®­ç»ƒï¼ˆPID: 763785ï¼‰

## å®ç°ç»†èŠ‚

### 1. AgentFlowæ–¹æ³•æ ¸å¿ƒè®¾è®¡

AgentFlowä½¿ç”¨GPT-4oä½œä¸ºLLM Judgeï¼Œç›´æ¥æ¯”è¾ƒå®Œæ•´å“åº”ä¸Ground Truthï¼Œè€Œéä¾èµ–ç­”æ¡ˆæå–ã€‚

**å…³é”®Promptè®¾è®¡**:
```python
query_prompt = f"""
You are a precise evaluator. Determine if the Model Response is equivalent to the Ground Truth.

**Instructions:**
1. **Extract:** Isolate the final answer from the Model Response, ignoring reasoning.
2. **Normalize & Compare:** The extracted answer and Ground Truth must be equivalent:
   - **Math:** Mathematically identical (e.g., \\frac{{1}}{{2}} == 0.5)
   - **Numbers/Text:** Ignore formatting, case, and currency/units.
3. **Verdict:** "True" only for semantically or mathematically equivalent answers.

**Inputs:**
Question: {question}
Model Response: {answer_extracted}
Ground Truth: {groundtruth}

**Format:**
<analysis>: Brief analysis
<true_false>: "True" or "False"
"""
```

### 2. æˆ‘ä»¬çš„å®ç°

**æ–‡ä»¶**: `src/reward_computer.py:68-186`

#### æ ¸å¿ƒç»„ä»¶

1. **åˆå§‹åŒ–LLM Judgeå®¢æˆ·ç«¯** (`_init_llm_judge_client`)
```python
self.llm_judge_client = OpenAI(
    base_url="http://localhost:8002/v1",
    api_key="sk-dummy"
)
self.llm_judge_model = "/home/yijia/lhy/openai/gpt-oss-120b"
```

2. **LLM Judgeæ¯”è¾ƒæ–¹æ³•** (`_llm_judge_compare`)
   - ä½¿ç”¨GPT OSS 120Bæ¨¡å‹ï¼ˆ120Bå‚æ•°ï¼Œæœ¬åœ°vLLMæœåŠ¡ï¼‰
   - Temperature = 0.0ï¼ˆç¡®å®šæ€§åˆ¤å†³ï¼‰
   - Max tokens = 200
   - çµæ´»çš„å“åº”è§£æï¼ˆæ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼‰

3. **æ”¹è¿›çš„å¥–åŠ±è®¡ç®—** (`compute_reward`)
```python
if self.use_llm_judge and problem_type != "code":
    # Mathå’ŒQAä»»åŠ¡ï¼šä½¿ç”¨LLM Judgeè¿›è¡Œè¯­ä¹‰æ¯”è¾ƒ
    is_correct = self._llm_judge_compare(
        problem=problem,
        prediction=str(prediction),
        ground_truth=str(ground_truth),
        problem_type=problem_type
    )
    correctness_score = 10.0 if is_correct else -5.0
else:
    # Codeä»»åŠ¡æˆ–å…œåº•ï¼šä½¿ç”¨ç­”æ¡ˆæå–+è§„åˆ™æ¯”è¾ƒ
    # ï¼ˆå› ä¸ºCodeä»»åŠ¡æœ‰æµ‹è¯•æ¡†æ¶ï¼Œä¸éœ€è¦LLM Judgeï¼‰
    ...
```

### 3. å“åº”è§£ææ”¹è¿›

**é—®é¢˜**: GPT OSS 120Bè¾“å‡ºæ ¼å¼ä¸ä¸€è‡´
- æœ‰æ—¶è¾“å‡º `<true_false>: True`
- æœ‰æ—¶è¾“å‡º `<true_false>True</true_false>`
- æœ‰æ—¶è¾“å‡º `**true_false**: True`

**è§£å†³æ–¹æ¡ˆ**: çµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
```python
true_false_match = re.search(
    r'(?:<true_false>|<true_false>:|\\*\\*true_false\\*\\*:?)\\s*(True|False)',
    result_text,
    re.IGNORECASE
)
```

### 4. æµ‹è¯•ç»“æœ

**æµ‹è¯•æ–‡ä»¶**: `test_llm_judge.py`

**æµ‹è¯•ç”¨ä¾‹**:
1. âœ… æ•°å­¦ - åˆ†æ•°ç­‰ä»· (0.5 == 1/2)
2. âœ… æ•°å­¦ - å®Œå…¨åŒ¹é… (42 == 42)
3. âœ… æ•°å­¦ - é”™è¯¯ç­”æ¡ˆ (50 != 42)
4. âœ… QA - è¯­ä¹‰ç­‰ä»· ("The capital of France is Paris" == "Paris")
5. âœ… QA - æ•°å€¼æå– ("He has 200 subscribers" == "200")
6. âœ… æ•°å­¦ - ä»£æ•°è¡¨è¾¾å¼ ("x^2+x-2" == "x^2+x-2")

**ç»“æœ**: 6/6é€šè¿‡ï¼Œ100%å‡†ç¡®ç‡

### 5. GRPOè®­ç»ƒå™¨é›†æˆ

**æ–‡ä»¶**: `src/grpo_trainer.py:197-207`

**ä¿®æ”¹**:
```python
self.reward_computer = RewardComputer(
    reward_weights=self.config.get('reward_weights'),
    use_llm_judge=True,  # å¯ç”¨LLM Judge
    llm_config={
        "base_url": "http://localhost:8002/v1",
        "api_key": "sk-dummy",
        "model_name": "/home/yijia/lhy/openai/gpt-oss-120b"
    }
)
```

## å½“å‰è®­ç»ƒçŠ¶æ€

**è¿›ç¨‹**: PID 763785
**æ—¥å¿—**: `logs/train_llm_judge_20251119_231710.log`
**GPU**: CUDA:2 (ç‰©ç†GPU 0)
**çŠ¶æ€**: Step 1/500è¿›è¡Œä¸­
**Batch**: 1 QA, 2 Math, 1 Code
**Temperature**: 0.305

**åˆå§‹åŒ–ç¡®è®¤**:
```
âœ… LLM Judgeå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ
   æ¨¡å‹: /home/yijia/lhy/openai/gpt-oss-120b
   URL: http://localhost:8002/v1
âœ… 10åˆ†åˆ¶å¥–åŠ±è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ
  æ¨¡å¼: æ­£ç¡®æ€§åˆ†æ•° [-10, 10] â†’ å½’ä¸€åŒ–å¥–åŠ± [0, 1]
  ç­”æ¡ˆæå–å™¨: å¯ç”¨
  LLM Judge: å¯ç”¨ (GPT OSS 120B @ port 8002)
```

## é¢„æœŸæ”¹å–„

### ä¸AgentFlowå¯¹æ¯”

| ç‰¹æ€§ | AgentFlow | æˆ‘ä»¬çš„å®ç° |
|------|-----------|------------|
| LLMæ¨¡å‹ | GPT-4o | GPT OSS 120B (120Bå‚æ•°) |
| éƒ¨ç½²æ–¹å¼ | APIè°ƒç”¨ | æœ¬åœ°vLLM (port 8002) |
| ä»»åŠ¡æ”¯æŒ | Math, QA, Code | Math, QAï¼ˆCodeä½¿ç”¨æµ‹è¯•æ¡†æ¶ï¼‰|
| å“åº”æ ¼å¼ | XMLæ ‡ç­¾ | çµæ´»è§£æå¤šç§æ ¼å¼ |
| å…œåº•æœºåˆ¶ | æ—  | ç­”æ¡ˆæå–+è§„åˆ™æ¯”è¾ƒ |

### é¢„æœŸæ•ˆæœ

ç›¸æ¯”ä¹‹å‰çš„ç­”æ¡ˆæå–+è§„åˆ™æ¯”è¾ƒï¼š

1. **Mathä»»åŠ¡**:
   - ä¿®å¤å‰: 37% â†’ 70-80%ï¼ˆç›®æ ‡ï¼‰
   - åŸå› : LLMèƒ½ç†è§£"x^2+x-2"ç­‰ä»£æ•°è¡¨è¾¾å¼ï¼Œé¿å…é”™è¯¯æå–"2"

2. **QAä»»åŠ¡**:
   - ä¿®å¤å‰: 0-25% â†’ 50-70%ï¼ˆç›®æ ‡ï¼‰
   - åŸå› : LLMèƒ½ä»é•¿æ–‡æœ¬ä¸­æå–æœ€ç»ˆç­”æ¡ˆï¼ˆå¦‚"200 subscribers"ï¼‰

3. **Codeä»»åŠ¡**:
   - ä¿æŒæµ‹è¯•æ¡†æ¶è¯„ä¼°ï¼ˆä¸ä½¿ç”¨LLM Judgeï¼‰
   - åŸå› : Codeæœ‰å‡†ç¡®çš„æµ‹è¯•ç”¨ä¾‹ï¼Œä¸éœ€è¦è¯­ä¹‰ç†è§£

## ç›‘æ§æ–¹æ³•

### å®æ—¶ç›‘æ§
```bash
tail -f logs/train_llm_judge_20251119_231710.log
```

### æŸ¥çœ‹LLM Judgeåˆ¤å†³
```bash
grep -A 5 'ğŸ¤– LLM Judgeç»“æœ' logs/train_llm_judge_*.log | tail -30
```

### æ£€æŸ¥å‡†ç¡®ç‡
```bash
grep 'å‡†ç¡®ç‡ç»Ÿè®¡' logs/train_llm_judge_*.log | tail -20
```

### wandbç›‘æ§
- **Project**: agent-prompt
- **Run**: quiet-dragon-63
- **URL**: https://wandb.ai/yao110002-sdfsdfsdfsdf-com/agent-prompt/runs/qd2x9c2y

## å…³é”®ä¼˜åŠ¿

### 1. é€šç”¨æ€§
- âœ… ä¸ä¾èµ–æ•°æ®é›†ç‰¹å®šæ ¼å¼ï¼ˆGSM8Kçš„`<<>>`ç­‰ï¼‰
- âœ… æ”¯æŒå¤šç§ç­”æ¡ˆå½¢å¼ï¼ˆåˆ†æ•°ã€å°æ•°ã€ä»£æ•°è¡¨è¾¾å¼ã€æ–‡æœ¬ï¼‰
- âœ… è‡ªåŠ¨å¤„ç†å•ä½è½¬æ¢ï¼ˆ1/2 == 0.5ï¼‰

### 2. é²æ£’æ€§
- âœ… LLMç†è§£è¯­ä¹‰ï¼Œä¸ä¾èµ–æ ¼å¼
- âœ… çµæ´»çš„å“åº”è§£æï¼Œå®¹å¿è¾“å‡ºæ ¼å¼å˜åŒ–
- âœ… å…œåº•æœºåˆ¶ï¼šLLMå¤±è´¥æ—¶é™çº§ä¸ºè§„åˆ™æ¯”è¾ƒ

### 3. æ€§èƒ½
- âœ… æœ¬åœ°vLLMéƒ¨ç½²ï¼Œä½å»¶è¿Ÿ
- âœ… 120Bå‚æ•°æ¨¡å‹ï¼Œå¼ºå¤§æ¨ç†èƒ½åŠ›
- âœ… Temperature=0ç¡®ä¿åˆ¤å†³ä¸€è‡´æ€§

## åç»­éªŒè¯é‡ç‚¹

1. **Step 1å®Œæˆæ—¶é—´**:
   - é¢„è®¡: ~15-20åˆ†é’Ÿ
   - ç›‘æ§LLM Judgeæ˜¯å¦å¢åŠ æ˜¾è‘—å»¶è¿Ÿ

2. **å‡†ç¡®ç‡æå‡**:
   - Math: æ˜¯å¦ä»37%æå‡åˆ°70%+
   - QA: æ˜¯å¦ä»25%æå‡åˆ°50%+

3. **LLM Judgeæ—¥å¿—**:
   - æ£€æŸ¥20%é‡‡æ ·çš„åˆ¤å†³æ—¥å¿—
   - ç¡®è®¤åˆ¤å†³åˆç†æ€§

4. **vLLMæœåŠ¡ç¨³å®šæ€§**:
   - ç›‘æ§port 8002æ˜¯å¦ç¨³å®šå“åº”
   - æ£€æŸ¥æ˜¯å¦æœ‰è¿æ¥é”™è¯¯

## ç›¸å…³æ–‡æ¡£

- `GENERAL_EXTRACTION_FIX.md` - Ground Truthæå–é€šç”¨æ–¹æ³•ä¿®å¤
- `BUG_FIXES_SUMMARY.md` - QAæå–å’ŒWorkflowç”Ÿæˆä¿®å¤
- `src/answer_extractor.py:21-113` - é€šç”¨ç­”æ¡ˆæå–æ–¹æ³•
- `src/reward_computer.py:68-186` - LLM Judgeå®ç°
- `test_llm_judge.py` - LLM Judgeæµ‹è¯•è„šæœ¬

## ä¿®æ”¹æ–‡ä»¶æ¸…å•

```
src/reward_computer.py:19-94     - RewardComputer.__init__ (æ–°å¢LLM Judgeå‚æ•°)
src/reward_computer.py:68-94     - _init_llm_judge_client (æ–°å¢)
src/reward_computer.py:96-186    - _llm_judge_compare (æ–°å¢)
src/reward_computer.py:180-260   - compute_reward (ä¿®æ”¹ä¸ºåŒæ¨¡å¼)
src/grpo_trainer.py:197-207      - å¯ç”¨LLM Judge
test_llm_judge.py                - LLM Judgeæµ‹è¯•è„šæœ¬ï¼ˆæ–°å¢ï¼‰
```

---

**ç”Ÿæˆæ—¶é—´**: 2025-11-19 23:18
**çŠ¶æ€**: âœ… LLM Judgeå·²å®ç°å¹¶å¯ç”¨ï¼Œè®­ç»ƒè¿›è¡Œä¸­
**PID**: 763785
**ä¸‹ä¸€æ­¥**: ç›‘æ§Step 1å®Œæˆï¼ŒéªŒè¯å‡†ç¡®ç‡æå‡æ•ˆæœ
