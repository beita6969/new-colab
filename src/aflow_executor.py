#!/usr/bin/env python3
"""
AFlowæ‰§è¡Œé€‚é…å™¨ - æ‰§è¡ŒRLç”Ÿæˆçš„å·¥ä½œæµ
"""
import sys
import os
import tempfile
import importlib.util
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
import asyncio
import time

# å¯¼å…¥å·¥ä½œæµéªŒè¯å™¨ã€å“åº”æ ‡å‡†åŒ–å™¨å’ŒSymPyä¿®å¤å™¨
try:
    from .workflow_validator import WorkflowValidator
    from .response_standardizer import ResponseStandardizer
    from .sympy_code_fixer import SymPyCodeFixer
except ImportError:
    from workflow_validator import WorkflowValidator
    from response_standardizer import ResponseStandardizer
    from sympy_code_fixer import SymPyCodeFixer

# æ·»åŠ AFlowåˆ°è·¯å¾„ï¼ˆæ·»åŠ å¤šä¸ªå¯èƒ½éœ€è¦çš„è·¯å¾„ï¼‰
aflow_path = '/home/yijia/.claude/11/AFlow'
sys.path.insert(0, aflow_path)
sys.path.insert(0, os.path.join(aflow_path, 'workspace'))

# å¯¼å…¥AFlowç»„ä»¶
from scripts.async_llm import create_llm_instance, LLMsConfig
from scripts import operators as operator_module

class AFlowExecutor:
    """æ‰§è¡ŒRLç”Ÿæˆçš„å·¥ä½œæµï¼Œä½¿ç”¨AFlowçš„ç®—å­"""

    def __init__(
        self,
        llm_config_path: str = "config/aflow_llm.yaml",
        llm_model_name: str = "gpt-oss-120b",  # ä½¿ç”¨8002ç«¯å£çš„gpt-oss-120b
        timeout: int = 300,
        operator_enhancer: Optional[Any] = None,
        enable_fallback: bool = True  # å¯ç”¨Fallbackæœºåˆ¶
    ):
        """
        Args:
            llm_config_path: AFlow LLMé…ç½®æ–‡ä»¶è·¯å¾„
            llm_model_name: ä½¿ç”¨çš„LLMæ¨¡å‹åç§°
            timeout: æ‰§è¡Œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            operator_enhancer: Layer 2 operatoræç¤ºè¯å¢å¼ºå™¨ï¼ˆå¯é€‰ï¼‰
            enable_fallback: æ˜¯å¦å¯ç”¨Fallbackæœºåˆ¶
        """
        self.llm_config_path = Path(llm_config_path)
        self.llm_model_name = llm_model_name
        self.timeout = timeout
        self.operator_enhancer = operator_enhancer
        self.enable_fallback = enable_fallback
        self.validator = WorkflowValidator()  # æ·»åŠ éªŒè¯å™¨
        self.standardizer = ResponseStandardizer()  # æ·»åŠ å“åº”æ ‡å‡†åŒ–å™¨
        self.sympy_fixer = SymPyCodeFixer()  # æ·»åŠ SymPyä¿®å¤å™¨

        # åŠ è½½LLMé…ç½®
        self._load_llm_config()

        print(f"âœ… AFlowæ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  LLMæ¨¡å‹: {llm_model_name}")
        print(f"  è¶…æ—¶: {timeout}ç§’")
        if operator_enhancer is not None:
            print(f"  Layer 2å¢å¼º: å¯ç”¨")

    def _load_llm_config(self):
        """åŠ è½½LLMé…ç½®"""
        try:
            # è®¾ç½®é…ç½®è·¯å¾„
            abs_config_path = self.llm_config_path.absolute()

            # è¯»å–YAMLé…ç½®æ–‡ä»¶
            import yaml
            with open(abs_config_path, 'r') as f:
                yaml_data = yaml.safe_load(f)

            # LLMsConfigæœŸæœ›çš„æ˜¯modelså­—å…¸
            models_config = yaml_data.get('models', {})

            # ä¸ºæœ¬åœ°vLLMæœåŠ¡ç¦ç”¨ä»£ç†
            import os
            if 'localhost' in str(models_config.get('gpt-oss-120b', {}).get('base_url', '')) or \
               '127.0.0.1' in str(models_config.get('gpt-oss-120b', {}).get('base_url', '')):
                os.environ['NO_PROXY'] = 'localhost,127.0.0.1'
                os.environ['no_proxy'] = 'localhost,127.0.0.1'
                print("  ğŸ“Œ è®¾ç½® NO_PROXY=localhost,127.0.0.1 (ç»•è¿‡ä»£ç†è®¿é—®vLLM)")

            # ç›´æ¥åŠ è½½é…ç½®
            from scripts.async_llm import LLMsConfig
            self.llm_configs = LLMsConfig(models_config)

            print(f"âœ… åŠ è½½LLMé…ç½®: {abs_config_path}")

        except Exception as e:
            print(f"âš ï¸  åŠ è½½LLMé…ç½®å¤±è´¥: {e}")
            print(f"  å°†ä½¿ç”¨ LLMsConfig.default()")
            # ä½¿ç”¨é»˜è®¤é…ç½®è€Œä¸æ˜¯ None
            from scripts.async_llm import LLMsConfig
            try:
                self.llm_configs = LLMsConfig.default()
                print(f"âœ… æˆåŠŸåŠ è½½é»˜è®¤LLMé…ç½®")
            except Exception as e2:
                print(f"  é»˜è®¤é…ç½®ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                # æœ€åçš„é™çº§æ–¹æ¡ˆï¼šè®¾ä¸º Noneï¼Œåç»­ç”¨å­—ç¬¦ä¸²
                self.llm_configs = None

    def validate_operator_output(self, output: Any, operator_name: str) -> Dict:
        """
        éªŒè¯å¹¶æ ‡å‡†åŒ–ç®—å­è¾“å‡ºæ ¼å¼ï¼ˆä½¿ç”¨ResponseStandardizerï¼‰

        Args:
            output: ç®—å­çš„åŸå§‹è¾“å‡º
            operator_name: ç®—å­åç§°

        Returns:
            æ ‡å‡†åŒ–åçš„è¾“å‡ºå­—å…¸
        """
        # ä½¿ç”¨ResponseStandardizerè¿›è¡Œæ ‡å‡†åŒ–
        standardized = self.standardizer.standardize(output, operator_name)

        # ä¿æŒå‘åå…¼å®¹ï¼ŒåŒæ—¶è¿”å›åŸå§‹å­—æ®µå’Œæ ‡å‡†åŒ–å­—æ®µ
        if isinstance(output, dict):
            result = output.copy()
            result.update({
                '__standardized__': standardized,
                # ç¡®ä¿å…³é”®å­—æ®µå­˜åœ¨
                'response': standardized['content'],
                'success': standardized['success'],
                'error': standardized.get('error')
            })
            return result
        else:
            return standardized

    async def execute_workflow(
        self,
        workflow_code: str,
        problem: str,
        problem_type: str = "math",
        **kwargs
    ) -> Tuple[Any, float, Dict]:
        """
        æ‰§è¡Œå·¥ä½œæµ

        Args:
            workflow_code: RLæ¨¡å‹ç”Ÿæˆçš„Workflowç±»ä»£ç 
            problem: é—®é¢˜æ–‡æœ¬
            problem_type: é—®é¢˜ç±»å‹
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚entry_point for codeï¼‰

        Returns:
            (answer, cost, metadata)
        """

        start_time = time.time()

        # 1. éªŒè¯å·¥ä½œæµä»£ç 
        is_valid, msg, validation_details = self.validator.validate_workflow_code(workflow_code, problem_type)

        if not is_valid:
            print(f"âš ï¸  å·¥ä½œæµä»£ç éªŒè¯å¤±è´¥: {msg}")

            # å°è¯•è‡ªåŠ¨ä¿®å¤
            fixed_code = self.validator.fix_common_issues(workflow_code)
            is_valid, msg, _ = self.validator.validate_workflow_code(fixed_code, problem_type)

            if is_valid:
                print(f"âœ… è‡ªåŠ¨ä¿®å¤æˆåŠŸ")
                workflow_code = fixed_code
            elif self.enable_fallback:
                print(f"  ä½¿ç”¨Fallbackå·¥ä½œæµ")
                return await self._execute_fallback_workflow(problem, problem_type, **kwargs)
            else:
                # Fallbackç¦ç”¨ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise ValueError(f"å·¥ä½œæµä»£ç æ— æ•ˆä¸”Fallbackå·²ç¦ç”¨: {msg}")

        # 2. ä¿®å¤SymPyå…¼å®¹æ€§é—®é¢˜ï¼ˆé’ˆå¯¹Codeç±»å‹ï¼‰
        if problem_type == "code" or 'sympy' in workflow_code.lower():
            fixed_code, was_modified, fixes = self.sympy_fixer.fix_code(workflow_code)
            if was_modified:
                print(f"ğŸ”§ SymPyä»£ç ä¿®å¤: {', '.join(fixes)}")
                workflow_code = fixed_code

        try:
            # åˆ›å»ºä¸´æ—¶å·¥ä½œæµæ¨¡å—
            workflow_class = self._create_workflow_class(workflow_code, problem_type)

            # å®ä¾‹åŒ–å·¥ä½œæµ
            llm_config = self._get_llm_config()

            # ç¡®ä¿ llm_config ä¸æ˜¯ None
            if llm_config is None:
                print(f"âš ï¸  llm_config ä¸º Noneï¼Œé™çº§ä¸ºå­—ç¬¦ä¸²: {self.llm_model_name}")
                llm_config = self.llm_model_name

            try:
                workflow = workflow_class(
                    name="rl_generated_workflow",
                    llm_config=llm_config,
                    dataset=problem_type
                )
            except Exception as e:
                # å·¥ä½œæµå®ä¾‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨fallback
                print(f"âš ï¸  å·¥ä½œæµå®ä¾‹åŒ–å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                print(f"  ä½¿ç”¨fallbackå·¥ä½œæµ")
                fallback_class = self._get_fallback_workflow_class(problem_type)
                workflow = fallback_class(
                    name="fallback_workflow",
                    llm_config=llm_config,
                    dataset=problem_type
                )

            # æ‰§è¡Œï¼ˆå¸¦è¶…æ—¶ï¼‰
            # For code problems, try passing entry_point and test (HumanEval format)
            try:
                if problem_type == "code":
                    # Try full HumanEval format first (entry_point + test)
                    if "entry_point" in kwargs and "test" in kwargs:
                        try:
                            result = await asyncio.wait_for(
                                workflow(problem, kwargs["entry_point"], kwargs["test"]),
                                timeout=self.timeout
                            )
                        except TypeError as e:
                            # Fallback to just entry_point
                            if "positional argument" in str(e) or "takes" in str(e):
                                print(f"  âš ï¸  Workflowä¸æ”¯æŒtestå‚æ•°ï¼Œå°è¯•åªä¼ entry_point")
                                try:
                                    result = await asyncio.wait_for(
                                        workflow(problem, kwargs["entry_point"]),
                                        timeout=self.timeout
                                    )
                                except TypeError:
                                    print(f"  âš ï¸  Workflowä¸æ”¯æŒentry_pointå‚æ•°ï¼Œé™çº§ä¸ºåªä¼ problem")
                                    result = await asyncio.wait_for(
                                        workflow(problem),
                                        timeout=self.timeout
                                    )
                            else:
                                raise
                    elif "entry_point" in kwargs:
                        # Only entry_point available
                        try:
                            result = await asyncio.wait_for(
                                workflow(problem, kwargs["entry_point"]),
                                timeout=self.timeout
                            )
                        except TypeError as e:
                            if "positional argument" in str(e):
                                print(f"  âš ï¸  Workflowä¸æ”¯æŒentry_pointå‚æ•°ï¼Œé™çº§ä¸ºåªä¼ problem")
                                result = await asyncio.wait_for(
                                    workflow(problem),
                                    timeout=self.timeout
                                )
                            else:
                                raise
                    else:
                        # No extra parameters
                        result = await asyncio.wait_for(
                            workflow(problem),
                            timeout=self.timeout
                        )
                else:
                    # Non-code problems
                    result = await asyncio.wait_for(
                        workflow(problem),
                        timeout=self.timeout
                    )
            except Exception as e:
                # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼ˆoperatoræ‰§è¡Œå¤±è´¥ï¼‰
                print(f"  âŒ Workflowæ‰§è¡Œå¼‚å¸¸: {type(e).__name__}")
                print(f"     å¼‚å¸¸ä¿¡æ¯: {str(e)}")
                import traceback
                print(f"  å®Œæ•´å †æ ˆ:")
                traceback.print_exc()

                # æ£€æŸ¥æ˜¯å¦å¯ç”¨Fallback
                if self.enable_fallback:
                    print(f"  ğŸ”„ å°è¯•ä½¿ç”¨Fallbackæœºåˆ¶")
                    return await self._execute_fallback_workflow(problem, problem_type, **kwargs)
                else:
                    print(f"  âš ï¸  Fallbackå·²ç¦ç”¨ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸")
                    # ç›´æ¥æŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯ä½¿ç”¨fallback
                    raise

            # å®‰å…¨åœ°è§£åŒ…ç»“æœï¼ˆå¯èƒ½è¿”å›2ä¸ªæˆ–æ›´å¤šå€¼ï¼‰
            if isinstance(result, tuple):
                if len(result) >= 2:
                    answer, cost = result[0], result[1]

                    # ç±»å‹éªŒè¯å’Œä¿®æ­£
                    if not isinstance(cost, (int, float)):
                        print(f"  è­¦å‘Š: costç±»å‹é”™è¯¯ ({type(cost).__name__})ï¼Œå°è¯•ä¿®æ­£...")
                        # æ£€æŸ¥æ˜¯å¦answerå’Œcostä½ç½®åäº†
                        if isinstance(answer, (int, float)) and isinstance(cost, str):
                            print(f"  æ£€æµ‹åˆ°answerå’Œcosté¡ºåºåè½¬ï¼Œäº¤æ¢...")
                            answer, cost = cost, answer
                        else:
                            # costæ˜¯å­—ç¬¦ä¸²ä½†ä¸æ˜¯æ•°å­—ï¼Œè®¾ä¸º0
                            print(f"  coståŒ…å«éæ•°å­—å†…å®¹ï¼Œè®¾ä¸º0.0")
                            if len(str(cost)) <= 100:
                                print(f"     costå†…å®¹: {cost}")
                            else:
                                print(f"     costå†…å®¹é¢„è§ˆ: {str(cost)[:100]}...")
                            cost = 0.0

                elif len(result) == 1:
                    answer, cost = result[0], 0.0
                else:
                    answer, cost = None, 0.0
            else:
                answer, cost = result, 0.0

            # æœ€ç»ˆç±»å‹ç¡®ä¿
            if not isinstance(cost, (int, float)):
                print(f"  costæœ€ç»ˆç±»å‹ä»ç„¶é”™è¯¯ï¼Œå¼ºåˆ¶è®¾ä¸º0.0")
                cost = 0.0

            execution_time = time.time() - start_time

            # å…ƒæ•°æ®
            metadata = {
                "success": True,
                "execution_time": execution_time,
                "cost": cost,
                "problem_type": problem_type
            }

            return answer, cost, metadata

        except asyncio.TimeoutError:
            execution_time = time.time() - start_time
            print(f"â±ï¸  æ‰§è¡Œè¶…æ—¶ ({self.timeout}ç§’)")

            metadata = {
                "success": False,
                "error": "timeout",
                "execution_time": execution_time,
                "cost": 0.0,
                "problem_type": problem_type
            }

            return None, 0.0, metadata

        except Exception as e:
            execution_time = time.time() - start_time
            print(f"âŒ æ‰§è¡Œé”™è¯¯: {str(e)}")

            import traceback
            traceback.print_exc()

            metadata = {
                "success": False,
                "error": str(e),
                "execution_time": execution_time,
                "cost": 0.0,
                "problem_type": problem_type
            }

            return None, 0.0, metadata

    def _create_workflow_class(self, workflow_code: str, problem_type: str):
        """ä»å·¥ä½œæµä»£ç åŠ¨æ€åˆ›å»ºWorkflowç±»"""

        # å‡†å¤‡å‘½åç©ºé—´
        namespace = {
            "operator": operator_module,
            "create_llm_instance": create_llm_instance,
            "DatasetType": str
        }

        # æ›¿æ¢importè·¯å¾„ï¼ˆä½¿workspaceè·¯å¾„å¯ç”¨ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨scriptsä¸­çš„operator
        modified_code = workflow_code.replace(
            f"import workspace.{problem_type}.workflows.template.operator as operator",
            "# operator already imported"
        )

        # ä¿®å¤å¸¸è§typoï¼ˆRLæ¨¡å‹å¯èƒ½äº§ç”Ÿçš„é”™è¯¯ï¼‰
        modified_code = modified_code.replace("async_lll", "async_llm")
        modified_code = modified_code.replace("create_lll_instance", "create_llm_instance")

        try:
            # æ‰§è¡Œä»£ç åˆ›å»ºç±»
            exec(modified_code, namespace)

            # è¿”å›Workflowç±»
            if "Workflow" not in namespace:
                raise ValueError("No Workflow class found in generated code")

            return namespace["Workflow"]

        except Exception as e:
            print(f"âš ï¸  ç”Ÿæˆçš„å·¥ä½œæµä»£ç æœ‰é”™è¯¯: {e}")
            print(f"  ä½¿ç”¨é»˜è®¤fallbackå·¥ä½œæµ")

            # ä½¿ç”¨ç®€å•çš„é»˜è®¤å·¥ä½œæµä½œä¸ºfallback
            return self._get_fallback_workflow_class(problem_type)

    def _get_llm_config(self):
        """è·å–LLMé…ç½®ï¼ˆç¡®ä¿è¿”å›æ­£ç¡®ç±»å‹ï¼‰"""
        from scripts.async_llm import LLMsConfig, LLMConfig

        try:
            if self.llm_configs:
                result = self.llm_configs.get(self.llm_model_name)
            else:
                # å°è¯•ä½¿ç”¨é»˜è®¤é…ç½®
                result = LLMsConfig.default().get(self.llm_model_name)

            # ç±»å‹éªŒè¯ï¼ˆå…³é”®ï¼ï¼‰
            if isinstance(result, LLMConfig):
                return result
            elif isinstance(result, dict):
                # å¦‚æœæ„å¤–è¿”å›äº† dictï¼Œè½¬æ¢ä¸º LLMConfig
                print(f"âš ï¸  è­¦å‘Šï¼šget() è¿”å›äº† dictï¼Œæ­£åœ¨è½¬æ¢ä¸º LLMConfig")
                return LLMConfig(result)
            elif isinstance(result, str):
                return result
            else:
                print(f"âš ï¸  æœªçŸ¥ç±»å‹: {type(result)}ï¼Œé™çº§ä¸ºå­—ç¬¦ä¸²")
                return self.llm_model_name

        except Exception as e:
            print(f"âš ï¸  è·å–LLMé…ç½®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›å­—ç¬¦ä¸²æ¨¡å‹åï¼Œè®© create_llm_instance è‡ªåŠ¨å¤„ç†
            print(f"  é™çº§ä¸ºå­—ç¬¦ä¸²æ¨¡å¼: {self.llm_model_name}")
            return self.llm_model_name

    async def _execute_fallback_workflow(
        self,
        problem: str,
        problem_type: str,
        **kwargs
    ) -> Tuple[Any, float, Dict]:
        """
        æ‰§è¡ŒFallbackå·¥ä½œæµ

        ä½¿ç”¨æœ€ç®€å•ä½†å¯é çš„æ–¹å¼æ‰§è¡Œ
        """
        print(f"ğŸ”„ æ‰§è¡ŒFallbackå·¥ä½œæµ")
        start_time = time.time()

        try:
            # ä½¿ç”¨ç®€å•çš„Customç®—å­
            if problem_type == "code":
                func_signature = ", entry_point"
            else:
                func_signature = ""

            simple_workflow_code = f'''
import asyncio

class Workflow:
    def __init__(self, name, llm_config, dataset):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem{func_signature}):
        """Simple fallback workflow using only Custom operator"""

        # Use Custom operator with appropriate instruction
        if self.dataset == "code":
            instruction = "Solve this coding problem. Provide a complete Python solution."
        elif self.dataset == "math":
            instruction = "Solve this math problem step by step. Show your work and provide the final answer."
        else:
            instruction = "Answer this question comprehensively."

        result = await self.custom(input=problem, instruction=instruction)

        # Validate and extract response
        if isinstance(result, dict):
            response = result.get("response", "")
        else:
            response = str(result)

        # Get cost
        try:
            cost = self.llm.get_usage_summary().get("total_cost", 0.0)
        except:
            cost = 0.0

        return response, cost
'''

            # åˆ›å»ºå·¥ä½œæµç±»
            workflow_class = self._create_workflow_class(simple_workflow_code, problem_type)

            # å®ä¾‹åŒ–
            llm_config = self._get_llm_config()
            workflow = workflow_class(
                name="fallback_workflow",
                llm_config=llm_config,
                dataset=problem_type
            )

            # æ‰§è¡Œ
            if problem_type == "code" and "entry_point" in kwargs:
                result = await asyncio.wait_for(
                    workflow(problem, kwargs["entry_point"]),
                    timeout=self.timeout
                )
            else:
                result = await asyncio.wait_for(
                    workflow(problem),
                    timeout=self.timeout
                )

            # è§£åŒ…ç»“æœ
            if isinstance(result, tuple) and len(result) >= 2:
                answer, cost = result[0], result[1]
            else:
                answer, cost = result, 0.0

            execution_time = time.time() - start_time

            metadata = {
                "success": True,
                "fallback_used": True,
                "execution_time": execution_time,
                "cost": cost,
                "problem_type": problem_type
            }

            print(f"âœ… FallbackæˆåŠŸ (è€—æ—¶: {execution_time:.2f}ç§’)")
            return answer, cost, metadata

        except Exception as e:
            execution_time = time.time() - start_time
            print(f"âŒ Fallbackä¹Ÿå¤±è´¥äº†: {e}")

            metadata = {
                "success": False,
                "fallback_used": True,
                "error": str(e),
                "execution_time": execution_time,
                "cost": 0.0,
                "problem_type": problem_type
            }

            # è¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return "", 0.0, metadata

    def _get_fallback_workflow_class(self, problem_type: str):
        """è¿”å›ä¸€ä¸ªç®€å•çš„é»˜è®¤å·¥ä½œæµç±»ï¼ˆç”¨äºç”Ÿæˆå¤±è´¥æ—¶ï¼‰

        æ”¹è¿›çš„fallbackç­–ç•¥ï¼š
        1. å…ˆå°è¯•ç›´æ¥è°ƒç”¨LLMç”Ÿæˆè§£å†³æ–¹æ¡ˆ
        2. å¦‚æœå¤±è´¥ï¼Œè¿”å›å ä½ç¬¦è€Œä¸æ˜¯None
        3. é¿å…ä¾èµ–å¯èƒ½å¤±è´¥çš„Test operator
        """

        class FallbackWorkflow:
            def __init__(self, name: str, llm_config, dataset):
                self.name = name
                self.dataset = dataset
                try:
                    self.llm = create_llm_instance(llm_config)
                except Exception as e:
                    print(f"âš ï¸  LLMåˆå§‹åŒ–å¤±è´¥: {e}")
                    self.llm = None

            async def __call__(self, problem: str, *args, **kwargs):
                """æ”¹è¿›çš„fallbackï¼šä¸ä¾èµ–Test operator"""

                # ç­–ç•¥1: ç›´æ¥è°ƒç”¨LLMç”Ÿæˆï¼Œä¸ç»è¿‡ä»»ä½•operator
                if self.llm is not None:
                    try:
                        print(f"  ğŸ“ Fallback: ç›´æ¥è°ƒç”¨LLMç”Ÿæˆè§£å†³æ–¹æ¡ˆ")

                        # æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©åˆé€‚çš„prompt
                        if self.dataset == "code":
                            prompt = f"""Given the following coding problem, provide a Python solution.

Problem:
{problem}

Provide ONLY the Python function code, no explanations."""
                        else:
                            prompt = f"""Solve the following problem step by step and provide the final answer.

Problem:
{problem}

Provide the final answer clearly."""

                        # ç›´æ¥è°ƒç”¨LLMï¼Œä¸ä½¿ç”¨ä»»ä½•operator
                        # ä½¿ç”¨æ­£ç¡®çš„ AsyncLLM __call__ æ¥å£
                        answer = await self.llm(prompt)

                        # è·å–æˆæœ¬
                        usage = self.llm.get_usage_summary()
                        if isinstance(usage, dict) and "total_cost" in usage:
                            cost = usage["total_cost"]
                        else:
                            cost = 0.0

                        return answer, cost

                    except Exception as e:
                        print(f"  âš ï¸  Fallbackç›´æ¥è°ƒç”¨LLMå¤±è´¥: {e}")

                # ç­–ç•¥2: å¦‚æœLLMè°ƒç”¨ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨Custom operatorä½†ä¸ä¾èµ–Test
                try:
                    print(f"  ğŸ“ Fallback: å°è¯•ä½¿ç”¨Custom operator")
                    custom = operator_module.Custom(self.llm)
                    result = await custom(
                        input=problem,
                        instruction="Generate a solution without requiring test validation."
                    )

                    if result and 'response' in result:
                        usage = self.llm.get_usage_summary()
                        if isinstance(usage, dict) and "total_cost" in usage:
                            cost = usage["total_cost"]
                        else:
                            cost = 0.0
                        return result['response'], cost

                except Exception as e:
                    print(f"  âš ï¸  Fallback Custom operatorå¤±è´¥: {e}")

                # ç­–ç•¥3: æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œè¿”å›å ä½ç¬¦è€Œä¸æ˜¯None
                print(f"  âš ï¸  æ‰€æœ‰fallbackç­–ç•¥éƒ½å¤±è´¥ï¼Œè¿”å›å ä½ç¬¦")
                placeholder = f"[Fallback placeholder for problem: {problem[:80]}...]"
                return placeholder, 0.0

        return FallbackWorkflow


async def test_executor():
    """æµ‹è¯•AFlowæ‰§è¡Œå™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•AFlowæ‰§è¡Œå™¨")
    print("=" * 60)

    # åˆ›å»ºæ‰§è¡Œå™¨
    executor = AFlowExecutor(
        llm_config_path="config/aflow_llm.yaml",
        llm_model_name="gpt-oss-120b",
        timeout=60
    )

    # æµ‹è¯•å·¥ä½œæµä»£ç ï¼ˆç®€å•ç¤ºä¾‹ï¼‰
    test_workflow_code = """
import workspace.math.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem: str):
        solution = await self.custom(input=problem, instruction="Solve this problem step by step and provide the final answer.")
        return solution['response'], self.llm.get_usage_summary()["total_cost"]
"""

    # æµ‹è¯•é—®é¢˜
    test_problem = "What is 15 + 27?"

    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {test_problem}")

    # æ‰§è¡Œå·¥ä½œæµ
    answer, cost, metadata = await executor.execute_workflow(
        workflow_code=test_workflow_code,
        problem=test_problem,
        problem_type="math"
    )

    print(f"\nâœ… æ‰§è¡Œç»“æœ:")
    print(f"  æˆåŠŸ: {metadata['success']}")
    print(f"  ç­”æ¡ˆ: {answer}")
    print(f"  æˆæœ¬: ${cost:.6f}")
    print(f"  æ—¶é—´: {metadata['execution_time']:.2f}ç§’")


if __name__ == "__main__":
    asyncio.run(test_executor())
