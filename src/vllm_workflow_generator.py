#!/usr/bin/env python3
"""
vLLMå·¥ä½œæµç”Ÿæˆå™¨ - ä½¿ç”¨vLLM APIè¿›è¡Œå¹¶å‘æ¨ç†ï¼ˆFallback: ä½¿ç”¨transformersï¼‰
"""
import asyncio
import torch
from openai import AsyncOpenAI
from typing import Dict, List, Optional, Tuple
import json
import ast
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer

class VLLMWorkflowGenerator:
    """ä½¿ç”¨vLLM APIç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµï¼ˆæ”¯æŒå¹¶å‘ï¼‰

    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. vLLM APIæ¨¡å¼ï¼ˆæ¨èï¼‰ï¼šé€šè¿‡AsyncOpenAIå®¢æˆ·ç«¯è°ƒç”¨vLLMæœåŠ¡
    2. Transformersæ¨¡å¼ï¼ˆFallbackï¼‰ï¼šç›´æ¥ä½¿ç”¨transformersåº“
    """

    def __init__(
        self,
        base_url: str = "http://localhost:8003/v1",
        api_key: str = "EMPTY",
        model_name: str = "/home/yijia/verl-agent/models/qwen/Qwen2___5-7B-Instruct",
        max_concurrent: int = 6,
        operator_descriptions_path: Optional[str] = None,
        config: Optional[Dict] = None,
        use_vllm_api: bool = False,  # é»˜è®¤ä½¿ç”¨transformersæ¨¡å¼
        device: str = "cuda:0"
    ):
        """
        Args:
            base_url: vLLMæœåŠ¡å™¨åœ°å€
            api_key: APIå¯†é’¥ï¼ˆvLLMä¸éœ€è¦çœŸå®å¯†é’¥ï¼‰
            model_name: æ¨¡å‹åç§°/è·¯å¾„
            max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
            operator_descriptions_path: AFlowç®—å­æè¿°æ–‡ä»¶è·¯å¾„
            config: é¢å¤–é…ç½®
            use_vllm_api: æ˜¯å¦ä½¿ç”¨vLLM APIï¼ˆFalseåˆ™ä½¿ç”¨transformersï¼‰
            device: è®¾å¤‡ï¼ˆtransformersæ¨¡å¼ï¼‰
        """
        self.model_name = model_name
        self.max_concurrent = max_concurrent
        self.config = config or {}
        self.use_vllm_api = use_vllm_api
        self.device = device

        # åŠ è½½ç®—å­æè¿°
        self.operator_descriptions = self._load_operator_descriptions(operator_descriptions_path)

        if use_vllm_api:
            # vLLM APIæ¨¡å¼
            self.client = AsyncOpenAI(
                base_url=base_url,
                api_key=api_key,
                timeout=300.0,  # 5åˆ†é’Ÿè¶…æ—¶
                max_retries=2
            )
            self.semaphore = asyncio.Semaphore(max_concurrent)
            print(f"âœ… åˆå§‹åŒ–vLLMå·¥ä½œæµç”Ÿæˆå™¨ï¼ˆAPIæ¨¡å¼ï¼‰")
            print(f"  æœåŠ¡å™¨: {base_url}")
            print(f"  æœ€å¤§å¹¶å‘: {max_concurrent}")
        else:
            # Transformersæ¨¡å¼ï¼ˆç›´æ¥ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹ï¼‰
            self.model = None  # å°†ç”±å¤–éƒ¨è®¾ç½®ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
            self.tokenizer = None
            # âš ï¸ å…³é”®ä¿®å¤ï¼šä½¿ç”¨é”ä¿æŠ¤GPUè®¿é—®ï¼ˆåŒä¸€æ—¶é—´åªå…è®¸ä¸€ä¸ªæ¨ç†ï¼‰
            self._generation_lock = asyncio.Lock()
            print(f"âœ… åˆå§‹åŒ–workflowç”Ÿæˆå™¨ï¼ˆTransformersæ¨¡å¼ï¼‰")
            print(f"  æ¨¡å‹: {model_name}")
            print(f"  è®¾å¤‡: {device}")
            print(f"  âš ï¸  GPUæ¨ç†å°†ä¸²è¡Œæ‰§è¡Œï¼ˆé¿å…CUDAå†²çªï¼‰")

    def _load_operator_descriptions(self, descriptions_path: Optional[str]) -> Dict:
        """åŠ è½½AFlowç®—å­æè¿°"""
        if descriptions_path and Path(descriptions_path).exists():
            with open(descriptions_path, 'r') as f:
                return json.load(f)

        # é»˜è®¤ç®—å­æè¿° - AFlowæ ‡å‡†10ä¸ªç®—å­
        return {
            "Custom": {
                "description": "Generates anything based on customized input and instruction.",
                "interface": "custom(input: str, instruction: str) -> dict with key 'response'"
            },
            "AnswerGenerate": {
                "description": "Generates step-by-step reasoning with thought process and final answer.",
                "interface": "answer_generate(input: str) -> dict with keys 'thought' and 'answer'"
            },
            "CustomCodeGenerate": {
                "description": "Generates code based on customized input and instruction.",
                "interface": "custom_code_generate(problem: str, entry_point: str, instruction: str) -> dict with key 'code'"
            },
            "Programmer": {
                "description": "Automatically writes and executes Python code, returns execution result.",
                "interface": "programmer(problem: str, analysis: str = 'None') -> dict with keys 'code' and 'output'"
            },
            "Test": {
                "description": "Tests code with test cases, reflects on errors and revises.",
                "interface": "test(problem: str, solution: str, entry_point: str, test_loop: int = 3) -> dict with keys 'result' and 'solution'"
            },
            "Format": {
                "description": "Extracts concise answer from verbose solution.",
                "interface": "format(problem: str, solution: str) -> dict with key 'solution'"
            },
            "Review": {
                "description": "Reviews solution correctness using critical thinking.",
                "interface": "review(problem: str, solution: str) -> dict with keys 'review_result' (bool) and 'feedback'"
            },
            "Revise": {
                "description": "Revises solution based on feedback.",
                "interface": "revise(problem: str, solution: str, feedback: str) -> dict with key 'solution'"
            },
            "ScEnsemble": {
                "description": "Uses self-consistency to select the most frequent solution.",
                "interface": "sc_ensemble(solutions: List[str], problem: str) -> dict with key 'response'"
            },
            "MdEnsemble": {
                "description": "Majority voting ensemble - shuffles and votes multiple times (more robust than ScEnsemble).",
                "interface": "md_ensemble(solutions: List[str], problem: str) -> dict with key 'solution'"
            }
        }

    def _build_generation_prompt(self, problem: str, problem_type: str) -> str:
        """æ„å»ºç”Ÿæˆæç¤ºè¯ - AFlowé£æ ¼XMLè¾“å‡ºæ ¼å¼"""
        prompt = f"""You are building a Workflow to solve {problem_type} problems.

## ğŸš¨ CRITICAL: OUTPUT FORMAT
You MUST output your workflow in XML format with <graph> and <prompt> tags:

<workflow>
<graph>
[Your Python Workflow class code here]
</graph>
<prompt>
[Your custom TASK_PROMPT here]
</prompt>
</workflow>

DO NOT:
- Directly answer the problem
- Output explanations without the XML tags
- Skip the <graph> or <prompt> sections

## Available Operators (AFlow Standard - 10 Operators)

1. **Custom(llm)** - Execute with custom instruction
   Call: await self.custom(input=str, instruction=str)
   Returns: {{'response': str}}

2. **AnswerGenerate(llm)** - Step-by-step reasoning with thought and answer
   Call: await self.answer_generate(input=str)
   Returns: {{'thought': str, 'answer': str}}

3. **CustomCodeGenerate(llm)** - Generate code with custom instruction
   Call: await self.custom_code_generate(problem=str, entry_point=str, instruction=str)
   Returns: {{'code': str}}

4. **Programmer(llm)** - Generate and execute Python code
   Call: await self.programmer(problem=str, analysis=str)
   Returns: {{'code': str (source), 'output': str (RESULT - USE THIS!)}}
   âš ï¸ CRITICAL: result['output'] = computed answer, result['code'] = source code

5. **Test(llm)** - Test code with test cases and reflect/revise
   Call: await self.test(problem=str, solution=str, entry_point=str)
   Returns: {{'result': bool, 'solution': str}}

6. **Format(llm)** - Extract concise answer from solution
   Call: await self.format(problem=str, solution=str)
   Returns: {{'solution': str}}

7. **Review(llm)** - Review solution correctness
   Call: await self.review(problem=str, solution=str)
   Returns: {{'review_result': bool, 'feedback': str}}

8. **Revise(llm)** - Revise solution based on feedback
   Call: await self.revise(problem=str, solution=str, feedback=str)
   Returns: {{'solution': str}}

9. **ScEnsemble(llm)** - Self-consistency ensemble voting
   Call: await self.sc_ensemble(solutions=list, problem=str)
   Returns: {{'response': str}}

10. **MdEnsemble(llm, vote_count=5)** - Majority voting with shuffling
    Call: await self.md_ensemble(solutions=list, problem=str)
    Returns: {{'solution': str}}

## Example Output:

<workflow>
<graph>
class Workflow:
    def __init__(self, name: str, llm_config, dataset):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.answer_generate = operator.AnswerGenerate(self.llm)
        self.review = operator.Review(self.llm)
        self.revise = operator.Revise(self.llm)

    async def __call__(self, problem: str, entry_point: str = "solve"):
        # Generate initial answer
        result = await self.answer_generate(input=problem)
        answer = result['answer']

        # Review and revise if needed
        review = await self.review(problem=problem, solution=answer)
        if not review['review_result']:
            revised = await self.revise(
                problem=problem,
                solution=answer,
                feedback=review['feedback']
            )
            answer = revised['solution']

        return answer, self.llm.get_usage_summary()["total_cost"]
</graph>
<prompt>
TASK_PROMPT = '''Solve this problem step by step.
Show your reasoning clearly and provide the final answer.
'''
</prompt>
</workflow>

## Design Guidelines:
- Combine 1-7 operators creatively
- Use Review+Revise for quality assurance
- Use ScEnsemble/MdEnsemble for multiple solutions
- Use Programmer for computational problems (always use result['output']!)
- Design TASK_PROMPT to guide the execution LLM

---

Problem: {problem}
Problem Type: {problem_type}

Generate your workflow in XML format:
"""
        return prompt

    async def generate_workflow(
        self,
        problem: str,
        problem_type: str = "math",
        temperature: float = 0.7,
        max_new_tokens: int = 2048,
        custom_prompt: Optional[str] = None
    ) -> Dict:
        """
        ç”Ÿæˆå•ä¸ªå·¥ä½œæµï¼ˆå¼‚æ­¥ï¼‰

        Returns:
            {
                "workflow_code": "Pythonä»£ç ",
                "valid": bool,
                "error": Optional[str],
                "metadata": {...}
            }
        """
        if self.use_vllm_api:
            return await self._generate_with_vllm_api(
                problem, problem_type, temperature, max_new_tokens, custom_prompt
            )
        else:
            return await self._generate_with_transformers(
                problem, problem_type, temperature, max_new_tokens, custom_prompt
            )

    async def _generate_with_vllm_api(
        self,
        problem: str,
        problem_type: str,
        temperature: float,
        max_tokens: int,
        custom_prompt: Optional[str]
    ) -> Dict:
        """ä½¿ç”¨vLLM APIç”Ÿæˆ"""
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘æ•°
            try:
                # æ„å»ºæç¤ºè¯
                prompt = custom_prompt or self._build_generation_prompt(problem, problem_type)

                # è°ƒç”¨vLLM API
                response = await self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "You are a workflow generation expert."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens,
                    top_p=self.config.get('top_p', 0.95),
                )

                # æå–ç”Ÿæˆçš„ä»£ç 
                generated_text = response.choices[0].message.content
                workflow_code, is_valid, error = self._parse_workflow_code(generated_text, problem_type)

                return {
                    "workflow_code": workflow_code,
                    "valid": is_valid,
                    "error": error,
                    "metadata": {
                        "tokens": response.usage.total_tokens if response.usage else 0,
                        "model": self.model_name
                    }
                }

            except Exception as e:
                return {
                    "workflow_code": "",
                    "valid": False,
                    "error": str(e),
                    "metadata": {}
                }

    async def _generate_with_transformers(
        self,
        problem: str,
        problem_type: str,
        temperature: float,
        max_new_tokens: int,
        custom_prompt: Optional[str]
    ) -> Dict:
        """ä½¿ç”¨transformersç”Ÿæˆï¼ˆä½¿ç”¨é”ä¿æŠ¤GPUè®¿é—®ï¼‰"""
        # âš ï¸ å…³é”®ï¼šä½¿ç”¨é”ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªæ¨ç†åœ¨æ‰§è¡Œ
        async with self._generation_lock:
            loop = asyncio.get_event_loop()

            def _sync_generate():
                """åŒæ­¥ç”Ÿæˆå‡½æ•°ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
                # æ„å»ºæç¤ºè¯
                prompt = custom_prompt or self._build_generation_prompt(problem, problem_type)

                # Tokenize
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

                # ç”Ÿæˆ
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=self.config.get('top_p', 0.95),
                        top_k=self.config.get('top_k', 50),
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id
                    )

                # è§£ç 
                generated_text = self.tokenizer.decode(
                    outputs[0][inputs['input_ids'].shape[1]:],
                    skip_special_tokens=True
                )

                return generated_text

            try:
                # åœ¨é»˜è®¤executorä¸­è¿è¡Œï¼ˆCPUå¯†é›†å‹æ“ä½œï¼‰
                generated_text = await loop.run_in_executor(None, _sync_generate)

                # è§£æè¾“å‡º
                workflow_code, is_valid, error = self._parse_workflow_code(generated_text, problem_type)

                return {
                    "workflow_code": workflow_code,
                    "valid": is_valid,
                    "error": error,
                    "metadata": {
                        "problem": problem,
                        "problem_type": problem_type,
                        "temperature": temperature
                    }
                }
            except Exception as e:
                return {
                    "workflow_code": "",
                    "valid": False,
                    "error": str(e),
                    "metadata": {}
                }

    def _parse_workflow_code(self, generated_text: str, problem_type: str) -> Tuple[str, bool, Optional[str]]:
        """è§£æç”Ÿæˆçš„æ–‡æœ¬ï¼Œæå–å¹¶éªŒè¯å·¥ä½œæµä»£ç ï¼ˆæ”¯æŒXMLæ ¼å¼å’Œæ—§æ ¼å¼ï¼‰"""
        import re

        # ğŸ”§ é¦–å…ˆå°è¯•æå–XMLæ ¼å¼
        graph_code, prompt_code = self._extract_xml_workflow(generated_text)

        if graph_code:
            # XMLæ ¼å¼è§£ææˆåŠŸ
            print(f"  ğŸ“ æ£€æµ‹åˆ°XMLæ ¼å¼å·¥ä½œæµ")
            code = graph_code.strip()

            # å¤„ç†prompt_code
            if prompt_code:
                prompt_custom_code = prompt_code.strip()
                print(f"  ğŸ“ ä»<prompt>æ ‡ç­¾æå–TASK_PROMPT")
            else:
                prompt_custom_code = self._get_default_prompt_custom(problem_type)
                print(f"  ğŸ“ ä½¿ç”¨é»˜è®¤PROMPT_CUSTOM (é—®é¢˜ç±»å‹: {problem_type})")
        else:
            # å›é€€åˆ°æ—§æ ¼å¼è§£æ
            print(f"  âš ï¸ æœªæ£€æµ‹åˆ°XMLæ ¼å¼ï¼Œå›é€€åˆ°ä¼ ç»Ÿè§£æ")
            code, prompt_custom_code = self._parse_legacy_format(generated_text, problem_type)
            if not code:
                return "", False, "No Workflow class found"

        # ç¡®ä¿prompt_customä»£ç åœ¨Workflowç±»ä¹‹å‰
        # æ£€æŸ¥codeæ˜¯å¦å·²åŒ…å«TASK_PROMPTå®šä¹‰
        if "TASK_PROMPT" not in code and prompt_custom_code:
            # åœ¨classä¹‹å‰æ·»åŠ 
            class_match = re.search(r'^class Workflow', code, re.MULTILINE)
            if class_match:
                code = prompt_custom_code + "\n\n" + code
            else:
                code = prompt_custom_code + "\n" + code

        # âš ï¸ Auto-Fixï¼šè‡ªåŠ¨ä¿®å¤ç¼ºå¤±çš„operatoråˆå§‹åŒ–
        code = self._validate_and_fix_workflow(code, problem_type)

        # éªŒè¯è¯­æ³•
        try:
            ast.parse(code)
            is_valid = True
            error = None
        except SyntaxError as e:
            is_valid = False
            error = f"Syntax error: {str(e)}"
            code = self._get_default_workflow(problem_type)

        return code, is_valid, error

    def _extract_xml_workflow(self, text: str) -> Tuple[str, str]:
        """ä»XMLæ ¼å¼æå–graphå’Œpromptä»£ç 

        Returns:
            (graph_code, prompt_code) - å¦‚æœæœªæ‰¾åˆ°XMLæ ¼å¼åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        import re

        graph_code = ""
        prompt_code = ""

        # å°è¯•æå– <graph>...</graph>
        graph_match = re.search(r'<graph>\s*([\s\S]*?)\s*</graph>', text)
        if graph_match:
            graph_code = graph_match.group(1).strip()

        # å°è¯•æå– <prompt>...</prompt>
        prompt_match = re.search(r'<prompt>\s*([\s\S]*?)\s*</prompt>', text)
        if prompt_match:
            prompt_code = prompt_match.group(1).strip()

        return graph_code, prompt_code

    def _parse_legacy_format(self, generated_text: str, problem_type: str) -> Tuple[str, str]:
        """è§£ææ—§æ ¼å¼ï¼ˆPythonä»£ç å—æˆ–ç›´æ¥classå®šä¹‰ï¼‰"""
        import re

        # æå–ä»£ç å—
        code_start = generated_text.find("```python")
        if code_start == -1:
            code_start = generated_text.find("class Workflow:")
            if code_start == -1:
                return "", ""
            code = generated_text[code_start:]
        else:
            code_start += len("```python\n")
            code_end = generated_text.find("```", code_start)
            code = generated_text[code_start:code_end] if code_end != -1 else generated_text[code_start:]

        code = code.strip()

        # è§£æå¹¶æå–prompt_customéƒ¨åˆ†
        prompt_custom_start = code.find("# === PROMPT_CUSTOM START ===")
        prompt_custom_end = code.find("# === PROMPT_CUSTOM END ===")

        prompt_custom_code = ""
        if prompt_custom_start != -1 and prompt_custom_end != -1:
            end_line_end = code.find("\n", prompt_custom_end)
            if end_line_end == -1:
                end_line_end = len(code)
            prompt_custom_code = code[prompt_custom_start:end_line_end + 1]
            # ç§»é™¤åŸä½ç½®çš„prompt_custom
            code = code[:prompt_custom_start] + code[end_line_end + 1:]
        else:
            # å°è¯•æ£€æµ‹TASK_PROMPTå˜é‡å®šä¹‰
            task_prompt_match = re.search(
                r'^(TASK_PROMPT\s*=\s*(?:"""[\s\S]*?"""|\'\'\' [\s\S]*?\'\'\'))',
                code,
                re.MULTILINE
            )
            if task_prompt_match:
                prompt_custom_code = task_prompt_match.group(1)
            else:
                prompt_custom_code = self._get_default_prompt_custom(problem_type)

        return code.strip(), prompt_custom_code

    def _get_default_prompt_custom(self, problem_type: str) -> str:
        """è·å–é»˜è®¤çš„TASK_PROMPT"""
        if problem_type == "math":
            return '''TASK_PROMPT = """Solve this mathematical problem step by step.
Show your reasoning clearly and provide the final numerical answer.
Format: First explain your approach, then show calculations, finally state the answer."""'''
        elif problem_type == "code":
            return '''TASK_PROMPT = """Write a Python function to solve this problem.
Requirements:
1. The function should be efficient and handle edge cases
2. Include proper input validation
3. Return the correct type as specified"""'''
        else:
            return '''TASK_PROMPT = """Solve this problem carefully.
Provide a clear, structured answer with reasoning."""'''

    def _validate_and_fix_workflow(self, code: str, problem_type: str) -> str:
        """éªŒè¯å¹¶è‡ªåŠ¨ä¿®å¤workflowä¸­ç¼ºå¤±çš„operatoråˆå§‹åŒ–

        Args:
            code: ç”Ÿæˆçš„workflowä»£ç 
            problem_type: é—®é¢˜ç±»å‹

        Returns:
            ä¿®å¤åçš„ä»£ç 
        """
        import re

        # 1. æå–__init__ä¸­å·²åˆå§‹åŒ–çš„operators
        initialized_ops = set()
        init_section = re.search(r'def __init__\([^)]+\):[\s\S]+?(?=\n    async def|\n    def|$)', code)
        if init_section:
            init_code = init_section.group(0)
            # åŒ¹é… self.xxx = operator.XXX(self.llm)
            init_patterns = re.findall(r'self\.(\w+)\s*=\s*operator\.(\w+)\(', init_code)
            for attr_name, op_name in init_patterns:
                initialized_ops.add(attr_name)

        # 2. æå–__call__ä¸­ä½¿ç”¨çš„operators
        used_ops = set()
        call_section = re.search(r'async def __call__\([^)]+\):[\s\S]+', code)
        if call_section:
            call_code = call_section.group(0)
            # åŒ¹é… await self.xxx(...)
            used_patterns = re.findall(r'await self\.(\w+)\(', call_code)
            for op_name in used_patterns:
                used_ops.add(op_name)

        # 3. æ‰¾å‡ºç¼ºå¤±çš„operators
        missing_ops = used_ops - initialized_ops

        if missing_ops:
            print(f"\nâš ï¸  æ£€æµ‹åˆ°ç¼ºå¤±çš„operatoråˆå§‹åŒ–: {missing_ops}")
            print(f"   å·²åˆå§‹åŒ–: {initialized_ops}")
            print(f"   å·²ä½¿ç”¨: {used_ops}")

            # 4. è‡ªåŠ¨æ·»åŠ ç¼ºå¤±çš„åˆå§‹åŒ–ä»£ç 
            # æ‰¾åˆ° self.llm = create_llm_instance(...) çš„ä½ç½®
            llm_init_match = re.search(r'(\s+)(self\.llm = create_llm_instance\([^)]+\))', code)
            if llm_init_match:
                indent = llm_init_match.group(1)
                llm_init_line = llm_init_match.group(2)

                # æ„å»ºç¼ºå¤±çš„åˆå§‹åŒ–ä»£ç 
                missing_inits = []
                for op_name in sorted(missing_ops):
                    # æ¨æ–­operatorç±»åï¼ˆé¦–å­—æ¯å¤§å†™+é©¼å³°å‘½åï¼‰
                    # answer_generate -> AnswerGenerate
                    # review -> Review
                    op_class_name = ''.join(word.capitalize() for word in op_name.split('_'))

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„operatorï¼ˆAFlowæ ‡å‡†10ä¸ªç®—å­ï¼‰
                    valid_operators = [
                        'Custom', 'AnswerGenerate', 'CustomCodeGenerate',
                        'Programmer', 'Test', 'Format',
                        'Review', 'Revise', 'ScEnsemble', 'MdEnsemble'
                    ]
                    if op_class_name in valid_operators:
                        missing_inits.append(f"{indent}self.{op_name} = operator.{op_class_name}(self.llm)")

                if missing_inits:
                    # åœ¨ self.llm = ... ä¹‹åæ’å…¥
                    insert_code = '\n' + '\n'.join(missing_inits)
                    code = code.replace(llm_init_line, llm_init_line + insert_code)
                    print(f"âœ… è‡ªåŠ¨æ·»åŠ äº† {len(missing_inits)} ä¸ªç¼ºå¤±çš„operatoråˆå§‹åŒ–")

        return code

    def _get_default_workflow(self, problem_type: str = "math") -> str:
        """é»˜è®¤å·¥ä½œæµ - åŒ…å«TASK_PROMPT"""
        # æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©åˆé€‚çš„é»˜è®¤prompt
        if problem_type == "math":
            task_prompt = '''"""Solve this mathematical problem step by step.
Show your complete reasoning process:
1. Identify what the problem is asking
2. List known information and variables
3. Apply relevant formulas or methods
4. Perform calculations carefully
5. State the final numerical answer clearly

IMPORTANT: Always verify your answer before providing it."""'''
        elif problem_type == "code":
            task_prompt = '''"""Write a Python function to solve this problem.
Requirements:
1. Handle all edge cases properly
2. Use efficient algorithms
3. Include proper input validation
4. Return the correct type as specified
5. Add brief comments for complex logic"""'''
        else:
            task_prompt = '''"""Solve this problem carefully and provide a clear answer.
Show your reasoning step by step."""'''

        return f"""# === PROMPT_CUSTOM START ===
TASK_PROMPT = {task_prompt}
# === PROMPT_CUSTOM END ===

import workspace.{problem_type}.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem: str, entry_point: str = "solve"):
        # entry_point used for code problems with Test operator
        solution = await self.custom(input=problem, instruction=TASK_PROMPT)
        return solution['response'], self.llm.get_usage_summary()["total_cost"]
"""

    async def generate_workflows_batch(
        self,
        problems: List[str],
        problem_types: List[str],
        temperatures: List[float],
        custom_prompts: Optional[List[str]] = None
    ) -> List[Dict]:
        """
        æ‰¹é‡å¹¶å‘ç”Ÿæˆå·¥ä½œæµï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨GPU batchæ¨ç†ï¼‰

        Args:
            problems: é—®é¢˜åˆ—è¡¨
            problem_types: é—®é¢˜ç±»å‹åˆ—è¡¨
            temperatures: æ¸©åº¦åˆ—è¡¨
            custom_prompts: è‡ªå®šä¹‰æç¤ºè¯åˆ—è¡¨

        Returns:
            ç»“æœåˆ—è¡¨
        """
        if self.use_vllm_api:
            # vLLM APIæ¨¡å¼ï¼šå¹¶å‘è°ƒç”¨
            tasks = []
            for i in range(len(problems)):
                task = self.generate_workflow(
                    problem=problems[i],
                    problem_type=problem_types[i],
                    temperature=temperatures[i],
                    custom_prompt=custom_prompts[i] if custom_prompts else None
                )
                tasks.append(task)

            results = await asyncio.gather(*tasks, return_exceptions=True)

            processed_results = []
            for result in results:
                if isinstance(result, Exception):
                    processed_results.append({
                        "workflow_code": "",
                        "valid": False,
                        "error": str(result),
                        "metadata": {}
                    })
                else:
                    processed_results.append(result)

            return processed_results
        else:
            # Transformersæ¨¡å¼ï¼šä½¿ç”¨GPU batchæ¨ç†ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰
            return await self._batch_generate_with_transformers(
                problems, problem_types, temperatures, custom_prompts
            )

    async def _batch_generate_with_transformers(
        self,
        problems: List[str],
        problem_types: List[str],
        temperatures: List[float],
        custom_prompts: Optional[List[str]]
    ) -> List[Dict]:
        """ä½¿ç”¨transformersæ‰¹é‡ç”Ÿæˆï¼ˆGPU batchæ¨ç†ï¼Œæ”¯æŒåˆ†æ‰¹ä»¥é™ä½æ˜¾å­˜ï¼‰"""
        loop = asyncio.get_event_loop()

        # ğŸ”§ æ˜¾å­˜ä¼˜åŒ–ï¼šåˆ†æ‰¹ç”Ÿæˆï¼Œæ¯æ‰¹æœ€å¤š8ä¸ªåºåˆ—
        MAX_BATCH_SIZE = 8  # æ¯æ‰¹æœ€å¤š8ä¸ªï¼Œé™ä½æ˜¾å­˜å³°å€¼

        def _sync_batch_generate(batch_prompts, batch_temp):
            """åŒæ­¥æ‰¹é‡ç”Ÿæˆå‡½æ•°ï¼ˆå•æ‰¹ï¼‰"""
            # æ‰¹é‡tokenizeï¼ˆå…³é”®ï¼špaddingå¯¹é½ï¼‰
            inputs = self.tokenizer(
                batch_prompts,
                return_tensors="pt",
                padding=True,  # å¯¹é½åˆ°æœ€é•¿åºåˆ—
                truncation=True,
                max_length=3072
            ).to(self.device)

            # æ‰¹é‡ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.config.get('max_new_tokens', 2048),
                    temperature=batch_temp,
                    top_p=self.config.get('top_p', 0.95),
                    top_k=self.config.get('top_k', 50),
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            # æ‰¹é‡è§£ç 
            generated_texts = self.tokenizer.batch_decode(
                outputs[:, inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )

            # ğŸ”§ æ˜¾å­˜ä¼˜åŒ–ï¼šåŠæ—¶æ¸…ç†
            del inputs, outputs
            torch.cuda.empty_cache()

            return generated_texts

        try:
            # æ„å»ºæ‰€æœ‰prompts
            all_prompts = []
            for i in range(len(problems)):
                if custom_prompts and custom_prompts[i]:
                    prompt = custom_prompts[i]
                else:
                    prompt = self._build_generation_prompt(problems[i], problem_types[i])
                all_prompts.append(prompt)

            # ğŸ”§ åˆ†æ‰¹å¤„ç†ä»¥é™ä½æ˜¾å­˜å³°å€¼
            all_generated_texts = []
            for batch_start in range(0, len(all_prompts), MAX_BATCH_SIZE):
                batch_end = min(batch_start + MAX_BATCH_SIZE, len(all_prompts))
                batch_prompts = all_prompts[batch_start:batch_end]
                batch_temp = temperatures[batch_start]  # å‡è®¾åŒæ‰¹temperatureç›¸åŒ

                print(f"  ğŸ”§ ç”Ÿæˆæ‰¹æ¬¡ {batch_start//MAX_BATCH_SIZE + 1}/{(len(all_prompts)-1)//MAX_BATCH_SIZE + 1} ({len(batch_prompts)}ä¸ªåºåˆ—)")

                # åœ¨çº¿ç¨‹æ± æ‰§è¡Œå•æ‰¹æ¨ç†
                batch_texts = await loop.run_in_executor(
                    None, _sync_batch_generate, batch_prompts, batch_temp
                )
                all_generated_texts.extend(batch_texts)

            # è§£ææ‰€æœ‰ç»“æœ
            results = []
            for i, generated_text in enumerate(all_generated_texts):
                workflow_code, is_valid, error = self._parse_workflow_code(
                    generated_text, problem_types[i]
                )
                results.append({
                    "workflow_code": workflow_code,
                    "valid": is_valid,
                    "error": error,
                    "metadata": {
                        "problem": problems[i],
                        "problem_type": problem_types[i],
                        "temperature": temperatures[i]
                    }
                })

            return results

        except Exception as e:
            # å‡ºé”™æ—¶è¿”å›ç©ºç»“æœ
            return [{
                "workflow_code": "",
                "valid": False,
                "error": str(e),
                "metadata": {}
            } for _ in problems]
