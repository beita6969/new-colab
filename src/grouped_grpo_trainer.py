#!/usr/bin/env python3
"""
åˆ†ç»„GRPOè®­ç»ƒå™¨ - æ¯ç»„å¤šé—®é¢˜åŠ æƒè¯„åˆ† + å¤šæ ·æ€§æ‰“ç ´å¹³å±€

æ ¸å¿ƒæ”¹è¿›:
1. æ¯ step åŒ…å« 3 ä¸ªé—®é¢˜ç»„ï¼ˆmath/qa/code å„ 1 ç»„ï¼‰
2. æ¯ç»„ 4 ä¸ªé—®é¢˜ï¼ˆ2 easy + 2 hardï¼‰
3. æ¯ä¸ª workflow åœ¨ç»„å†…æ‰€æœ‰é—®é¢˜ä¸Šè¯„åˆ†ï¼ŒåŠ æƒè®¡ç®—
4. ç›¸åŒåˆ†æ•°æ—¶ç”¨å¤šæ ·æ€§æ‰“ç ´å¹³å±€ï¼Œä¿è¯æœ‰æ¢¯åº¦
"""

import os
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)
os.environ.pop('HTTP_PROXY', None)
os.environ.pop('HTTPS_PROXY', None)
os.environ['no_proxy'] = 'localhost,127.0.0.1'

import gc
import torch
import torch.nn.functional as F
import asyncio
import numpy as np
import yaml
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
import time
import json
import wandb

from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer, get_cosine_schedule_with_warmup

# å¯¼å…¥æ–°çš„åˆ†ç»„æ•°æ®ç®¡ç†å™¨å’Œå¥–åŠ±è®¡ç®—å™¨
from grouped_data_manager import GroupedDataManager
from grouped_reward import GroupedRewardCalculator
from vllm_workflow_generator import VLLMWorkflowGenerator
from aflow_executor import AFlowExecutor
from reward_computer import RewardComputer
from gpu_manager import GPUManager
from experience_buffer import ExperienceBuffer
from prompt_optimizer import PromptOptimizer
from operator_prompt_enhancer import OperatorPromptEnhancer


class GroupedGRPOTrainer:
    """åˆ†ç»„GRPOè®­ç»ƒå™¨ï¼šæ¯ç»„å¤šé—®é¢˜è¯„åˆ†ï¼Œç¡®ä¿æ¯ step åŒ…å«ä¸‰ç§ç±»å‹"""

    def __init__(self, config_path: str = "config/grouped_training.yaml"):
        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)

        print("=" * 60)
        print("ğŸš€ åˆå§‹åŒ–åˆ†ç»„GRPOè®­ç»ƒå™¨")
        print("=" * 60)

        # GPUç®¡ç†
        physical_gpus = self.config.get('physical_gpus', self.config['device_mapping'])
        self.gpu_manager = GPUManager(
            target_gpus=physical_gpus,
            protected_pids=self.config.get('protected_pids', []),
            auto_clean=False
        )
        print(f"âœ… ä½¿ç”¨GPU {physical_gpus}")

        # Temperature scheduling
        temp_config = self.config.get('temperature_schedule', {})
        self.temp_schedule = {
            'enabled': temp_config.get('enabled', True),
            'initial': temp_config.get('initial', 0.5),
            'final': temp_config.get('final', 0.15),
            'warmup_steps': temp_config.get('warmup_steps', 150)
        }

        # åˆå§‹åŒ–wandb
        self._initialize_wandb()

        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()

        print("=" * 60)
        print("âœ… åˆ†ç»„GRPOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print("=" * 60)

    def _initialize_wandb(self):
        """åˆå§‹åŒ–wandb"""
        wandb_config = self.config.get('wandb', {})
        wandb_api_key = wandb_config.get('api_key', '')

        try:
            if wandb_api_key and len(wandb_api_key) == 40:
                wandb.login(key=wandb_api_key)
                mode = "online"
            else:
                print("âš ï¸  wandb API keyæ— æ•ˆï¼Œä½¿ç”¨offlineæ¨¡å¼")
                mode = "offline"
        except Exception as e:
            print(f"âš ï¸  wandbç™»å½•å¤±è´¥: {e}")
            mode = "offline"

        wandb.init(
            project=wandb_config.get('project', 'grouped-grpo'),
            name=wandb_config.get('run_name', f"grouped-{time.strftime('%Y%m%d-%H%M%S')}"),
            mode=mode,
            config={
                "base_model": self.config['base_model'],
                "learning_rate": self.config['learning_rate'],
                "groups_per_domain": self.config.get('groups_per_domain', 1),
                "num_sequences": self.config['num_return_sequences_in_group'],
                "weight_easy": self.config.get('grouped_reward', {}).get('weight_easy', 0.3),
                "weight_hard": self.config.get('grouped_reward', {}).get('weight_hard', 0.7),
            }
        )
        print(f"âœ… wandbåˆå§‹åŒ–å®Œæˆ (mode: {mode})")

    def _initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""

        # 1. åˆ†ç»„æ•°æ®ç®¡ç†å™¨
        print("\nğŸ“‚ åˆå§‹åŒ–åˆ†ç»„æ•°æ®ç®¡ç†å™¨...")
        self.data_manager = GroupedDataManager(
            data_dir=self.config.get('grouped_data_dir', 'data/grouped'),
            groups_per_domain=self.config.get('groups_per_domain', 1),
            shuffle=True
        )
        self.data_manager.initialize()

        # 2. åˆ†ç»„å¥–åŠ±è®¡ç®—å™¨
        print("\nğŸ¯ åˆå§‹åŒ–åˆ†ç»„å¥–åŠ±è®¡ç®—å™¨...")
        reward_config = self.config.get('grouped_reward', {})
        self.grouped_reward = GroupedRewardCalculator(
            weight_easy=reward_config.get('weight_easy', 0.3),
            weight_hard=reward_config.get('weight_hard', 0.7),
            diversity_threshold=reward_config.get('diversity_threshold', 0.05),
            diversity_weight=reward_config.get('diversity_weight', 0.1),
            debug=self.config.get('debug', False)
        )
        print(f"  Easyæƒé‡: {reward_config.get('weight_easy', 0.3):.0%}")
        print(f"  Hardæƒé‡: {reward_config.get('weight_hard', 0.7):.0%}")
        print(f"  å¤šæ ·æ€§é˜ˆå€¼: {reward_config.get('diversity_threshold', 0.05)}")

        # 3. RLæ¨¡å‹
        print("\nğŸ¤– åŠ è½½RLæ¨¡å‹...")
        self._load_rl_model()

        # 4. Workflowç”Ÿæˆå™¨
        print("\nğŸ”§ åˆå§‹åŒ–Workflowç”Ÿæˆå™¨...")
        self.generator = VLLMWorkflowGenerator(
            model_name=self.config['base_model'],
            max_concurrent=self.config['num_return_sequences_in_group'],
            operator_descriptions_path=self.config.get('aflow_operator_descriptions_path'),
            use_vllm_api=False,
            device=f"cuda:{self.config['device_mapping'][0]}"
        )
        self.generator.model = self.model
        self.generator.tokenizer = self.tokenizer

        # 5. ExperienceBuffer
        print("\nğŸ“š åˆå§‹åŒ–ExperienceBuffer...")
        exp_config = self.config.get('experience_buffer', {})
        self.experience_buffer = ExperienceBuffer(
            buffer_size=exp_config.get('buffer_size', 100),
            reward_threshold=exp_config.get('reward_threshold', 8.0),
            persistence_dir=exp_config.get('persistence_dir', 'data/experience_buffer'),
            problem_types=["math", "code", "qa"]
        )

        # 6. PromptOptimizer
        print("\nâœ¨ åˆå§‹åŒ–PromptOptimizer...")
        self.prompt_optimizer = PromptOptimizer()
        self.use_dynamic_prompts = self.config.get('prompt_optimizer', {}).get('enabled', True)

        # 7. OperatorPromptEnhancer
        print("\nğŸ”§ åˆå§‹åŒ–OperatorPromptEnhancer...")
        self.operator_enhancer = OperatorPromptEnhancer(
            enable_enhancement=self.config.get('operator_prompt_enhancer', {}).get('enabled', True)
        )

        # 8. AFlowæ‰§è¡Œå™¨
        print("\nâš™ï¸  åˆå§‹åŒ–AFlowæ‰§è¡Œå™¨...")
        self.executor = AFlowExecutor(
            llm_config_path=self.config['aflow_config_path'],
            timeout=self.config.get('execution_timeout', 600),
            operator_enhancer=self.operator_enhancer
        )

        # 9. åŸºç¡€å¥–åŠ±è®¡ç®—å™¨ï¼ˆç”¨äºå•é—®é¢˜è¯„åˆ†ï¼‰
        print("\nğŸ¯ åˆå§‹åŒ–åŸºç¡€å¥–åŠ±è®¡ç®—å™¨...")
        self.reward_computer = RewardComputer(
            reward_weights=self.config.get('reward_weights'),
            use_llm_judge=True,
            llm_config={
                "base_url": "https://api.openai.com/v1",
                "api_key": os.environ.get('OPENAI_API_KEY', 'sk-dummy'),
                "model_name": "gpt-4o-mini"
            },
            debug_logging=self.config.get('debug', False)
        )

        # 10. ä¼˜åŒ–å™¨
        print("\nğŸ”¬ åˆå§‹åŒ–ä¼˜åŒ–å™¨...")
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config.get('weight_decay', 0.01)
        )

        # 11. å­¦ä¹ ç‡è°ƒåº¦å™¨
        warmup_steps = self.config.get('warmup_steps', 100)
        max_steps = self.config['max_steps']
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=max_steps
        )

    def _load_rl_model(self):
        """åŠ è½½RLæ¨¡å‹"""
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config['base_model'],
            torch_dtype=torch.bfloat16 if self.config.get('bf16', True) else torch.float16,
            device_map=f"cuda:{self.config['device_mapping'][0]}",
            trust_remote_code=True
        )

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config['base_model'],
            trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # æ¢¯åº¦æ£€æŸ¥ç‚¹
        if self.config.get('gradient_checkpointing', True):
            self.model.gradient_checkpointing_enable()
            print("âœ… æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")

        # LoRA
        if self.config.get('use_lora', True):
            lora_config = LoraConfig(
                r=self.config['lora_rank'],
                lora_alpha=self.config['lora_alpha'],
                target_modules=self.config['lora_target_modules'].split(','),
                lora_dropout=self.config.get('lora_dropout', 0.05),
                bias="none",
                task_type="CAUSAL_LM"
            )
            self.model = get_peft_model(self.model, lora_config)
            print("âœ… LoRAåº”ç”¨å®Œæˆ")
            self.model.print_trainable_parameters()

    def get_current_temperature(self, step: int) -> float:
        """è·å–å½“å‰æ¸©åº¦"""
        if not self.temp_schedule['enabled']:
            return self.config['generation_config']['temperature']

        if step < self.temp_schedule['warmup_steps']:
            progress = step / self.temp_schedule['warmup_steps']
            temp = (self.temp_schedule['initial'] +
                   progress * (self.temp_schedule['final'] - self.temp_schedule['initial']))
        else:
            temp = self.temp_schedule['final']

        return temp

    async def train_step(self, step: int) -> Dict:
        """
        å•æ­¥åˆ†ç»„GRPOè®­ç»ƒ

        æµç¨‹:
        1. é‡‡æ · 3 ä¸ªé—®é¢˜ç»„ï¼ˆmath/qa/code å„ 1 ç»„ï¼‰
        2. ä¸ºæ¯ç»„ç”Ÿæˆ K ä¸ª workflow
        3. æ¯ä¸ª workflow åœ¨ç»„å†… 4 ä¸ªé—®é¢˜ä¸Šè¯„åˆ†
        4. è®¡ç®—åŠ æƒå¾—åˆ† + å¤šæ ·æ€§æ‰“ç ´å¹³å±€
        5. è®¡ç®—ä¼˜åŠ¿å€¼ï¼Œæ›´æ–°æ¨¡å‹
        """
        torch.cuda.reset_peak_memory_stats()

        # 1. é‡‡æ ·é—®é¢˜ç»„ï¼ˆç¡®ä¿åŒ…å«ä¸‰ç§ç±»å‹ï¼‰
        groups = self.data_manager.sample_step_groups(split="train")
        stats = self.data_manager.get_step_stats(groups)

        print(f"\n{'='*60}")
        print(f"ğŸ“ Step {step}/{self.config['max_steps']}")
        print(f"{'='*60}")
        print(f"ğŸ“¦ é‡‡æ · {stats['total_groups']} ç»„ ({stats['total_problems']} é—®é¢˜)")
        print(f"   ç»„åˆ†å¸ƒ: {stats['groups']}")
        print(f"   é—®é¢˜åˆ†å¸ƒ: {stats['problems']}")

        current_temp = self.get_current_temperature(step)
        print(f"ğŸŒ¡ï¸  Temperature: {current_temp:.3f}")

        num_sequences = self.config['num_return_sequences_in_group']  # K

        # æ”¶é›†æ‰€æœ‰ç»“æœ
        all_rewards = []
        all_log_probs = []
        all_advantages = []
        step_metrics = {
            'rewards': [],
            'correctness': {'easy': [], 'hard': []},
            'diversity_scores': [],
            'by_domain': {'math': [], 'qa': [], 'code': []}
        }

        # 2. å¤„ç†æ¯ä¸ªé—®é¢˜ç»„
        for group_idx, group in enumerate(groups):
            group_id = group['group_id']
            domain = group['domain']
            problems = group['problems']

            print(f"\nğŸ“‚ å¤„ç†ç»„ {group_idx+1}/{len(groups)}: {group_id} ({domain})")

            # 2.1 ä¸ºè¯¥ç»„ç”Ÿæˆ K ä¸ª workflow
            # ä½¿ç”¨ç»„çš„ç¬¬ä¸€ä¸ªé—®é¢˜ä½œä¸ºä»£è¡¨æ€§è¾“å…¥
            representative_problem = problems[0]['question']

            # ç”Ÿæˆ workflow prompt
            if self.use_dynamic_prompts:
                custom_prompt = self.prompt_optimizer.build_dynamic_prompt(
                    problem=representative_problem,
                    problem_type=domain
                )
            else:
                custom_prompt = None

            workflow_results = await self.generator.generate_workflows_batch(
                problems=[representative_problem] * num_sequences,
                problem_types=[domain] * num_sequences,
                temperatures=[current_temp] * num_sequences,
                custom_prompts=[custom_prompt] * num_sequences if custom_prompt else None
            )

            workflows = [r['workflow_code'] for r in workflow_results]

            # 2.2 æ¯ä¸ª workflow åœ¨ç»„å†…æ‰€æœ‰é—®é¢˜ä¸Šè¯„åˆ†
            problem_scores_per_workflow = [[] for _ in range(num_sequences)]

            for prob_idx, problem in enumerate(problems):
                difficulty = problem['difficulty']
                weight = problem['weight']
                question = problem['question']
                answer = problem['answer']

                print(f"  ğŸ“ é—®é¢˜ {prob_idx+1}/{len(problems)} ({difficulty}, weight={weight:.2f})")

                for wf_idx, workflow_code in enumerate(workflows):
                    # æ‰§è¡Œ workflow
                    try:
                        pred_answer, cost, exec_meta = await self.executor.execute_workflow(
                            workflow_code=workflow_code,
                            problem=question,
                            problem_type=domain,
                            entry_point=problem.get('entry_point', ''),
                            test=problem.get('test_cases', []),
                            source=problem.get('source', domain),
                            context=problem.get('context', '')
                        )

                        # è®¡ç®—æ­£ç¡®æ€§
                        if exec_meta.get('success', False):
                            correctness = self.reward_computer.compute_reward(
                                problem=question,
                                prediction=pred_answer,
                                ground_truth=answer,
                                problem_type=domain,
                                metadata=exec_meta,
                                test=problem.get('test_cases', []),
                                entry_point=problem.get('entry_point', ''),
                                source=problem.get('source', domain)
                            )
                        else:
                            correctness = 0.0

                    except Exception as e:
                        print(f"    âš ï¸  WF{wf_idx+1} æ‰§è¡Œå¤±è´¥: {e}")
                        correctness = 0.0

                    # è®°å½•é—®é¢˜å¾—åˆ†
                    problem_scores_per_workflow[wf_idx].append({
                        'problem_id': problem['id'],
                        'difficulty': difficulty,
                        'weight': weight,
                        'correctness': correctness
                    })

                    # ç»Ÿè®¡
                    step_metrics['correctness'][difficulty].append(correctness)

            # 2.3 è®¡ç®—ç»„å†…å¥–åŠ±ï¼ˆåŠ æƒ + å¤šæ ·æ€§ï¼‰
            group_rewards, diag = self.grouped_reward.calculate_group_rewards(
                workflows=workflows,
                problem_scores_per_workflow=problem_scores_per_workflow
            )

            print(f"\n  ğŸ¯ ç»„ {group_id} å¥–åŠ±:")
            print(f"     åŠ æƒåˆ†: {[f'{s:.3f}' for s in diag['weighted_scores']]}")
            print(f"     å¤šæ ·æ€§: {[f'{s:.3f}' for s in diag['diversity_scores']]}")
            print(f"     æœ€ç»ˆå¥–åŠ±: {[f'{r:.3f}' for r in group_rewards]}")
            print(f"     éœ€è¦å¤šæ ·æ€§tiebreak: {diag['need_diversity_tiebreak']}")

            # 2.4 è®¡ç®—ä¼˜åŠ¿å€¼
            advantages = self.grouped_reward.compute_advantages(group_rewards)
            print(f"     ä¼˜åŠ¿å€¼: {[f'{a:.3f}' for a in advantages]}")

            # 2.5 è®¡ç®— log prob
            for wf_idx, workflow_code in enumerate(workflows):
                log_prob = await self._compute_log_prob(
                    representative_problem, workflow_code, domain
                )
                all_log_probs.append(log_prob)
                all_rewards.append(group_rewards[wf_idx])
                all_advantages.append(advantages[wf_idx])

            # ç»Ÿè®¡
            step_metrics['rewards'].extend(group_rewards)
            step_metrics['diversity_scores'].extend(diag['diversity_scores'])
            step_metrics['by_domain'][domain].extend(group_rewards)

        # 3. æ¢¯åº¦æ›´æ–°
        if len(all_advantages) > 0 and any(a != 0 for a in all_advantages):
            loss = self._compute_grpo_loss(
                log_probs=all_log_probs,
                advantages=all_advantages
            )

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.config.get('max_grad_norm', 1.0)
            )

            self.optimizer.step()
            self.scheduler.step()

            loss_value = loss.item()
        else:
            loss_value = 0.0
            print("âš ï¸  æ‰€æœ‰ä¼˜åŠ¿å€¼ä¸ºé›¶ï¼Œè·³è¿‡æ¢¯åº¦æ›´æ–°")

        # 4. æ—¥å¿—
        metrics = {
            'step': step,
            'loss': loss_value,
            'mean_reward': np.mean(step_metrics['rewards']) if step_metrics['rewards'] else 0,
            'mean_advantage': np.mean(all_advantages) if all_advantages else 0,
            'std_advantage': np.std(all_advantages) if all_advantages else 0,
            'easy_correctness': np.mean(step_metrics['correctness']['easy']) if step_metrics['correctness']['easy'] else 0,
            'hard_correctness': np.mean(step_metrics['correctness']['hard']) if step_metrics['correctness']['hard'] else 0,
            'mean_diversity': np.mean(step_metrics['diversity_scores']) if step_metrics['diversity_scores'] else 0,
            'lr': self.scheduler.get_last_lr()[0],
            'temperature': current_temp
        }

        # åˆ†åŸŸç»Ÿè®¡
        for domain in ['math', 'qa', 'code']:
            if step_metrics['by_domain'][domain]:
                metrics[f'{domain}_reward'] = np.mean(step_metrics['by_domain'][domain])

        wandb.log(metrics)

        print(f"\nğŸ“Š Step {step} æ€»ç»“:")
        print(f"   Loss: {loss_value:.4f}")
        print(f"   å¹³å‡å¥–åŠ±: {metrics['mean_reward']:.3f}")
        print(f"   Easyæ­£ç¡®ç‡: {metrics['easy_correctness']:.3f}")
        print(f"   Hardæ­£ç¡®ç‡: {metrics['hard_correctness']:.3f}")
        print(f"   ä¼˜åŠ¿std: {metrics['std_advantage']:.4f}")

        # æ¸…ç†
        gc.collect()
        torch.cuda.empty_cache()

        return metrics

    async def _compute_log_prob(
        self,
        problem: str,
        workflow_code: str,
        problem_type: str
    ) -> torch.Tensor:
        """è®¡ç®— workflow çš„ log æ¦‚ç‡"""
        # æ„å»ºè¾“å…¥
        input_text = f"Problem type: {problem_type}\nProblem: {problem}\n\n"
        full_text = input_text + workflow_code

        # tokenize
        inputs = self.tokenizer(
            full_text,
            return_tensors="pt",
            truncation=True,
            max_length=self.config['prompt_max_length'] + self.config['response_max_length']
        )
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        input_ids = inputs['input_ids']
        prompt_len = len(self.tokenizer.encode(input_text))

        # å‰å‘ä¼ æ’­ - P14ä¿®å¤: ç§»é™¤torch.no_grad()ä»¥ä¿ç•™æ¢¯åº¦ç”¨äºè®­ç»ƒ
        outputs = self.model(**inputs)
        logits = outputs.logits

        # è®¡ç®—ç”Ÿæˆéƒ¨åˆ†çš„ log prob
        shift_logits = logits[:, prompt_len-1:-1, :]
        shift_labels = input_ids[:, prompt_len:]

        log_probs = F.log_softmax(shift_logits, dim=-1)
        token_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)

        return token_log_probs.sum()

    def _compute_grpo_loss(
        self,
        log_probs: List[torch.Tensor],
        advantages: List[float]
    ) -> torch.Tensor:
        """è®¡ç®— GRPO æŸå¤±"""
        losses = []
        for log_prob, advantage in zip(log_probs, advantages):
            if advantage != 0:
                loss = -advantage * log_prob
                losses.append(loss)

        if losses:
            return torch.stack(losses).mean()
        return torch.tensor(0.0, device=self.model.device)

    async def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("\n" + "="*60)
        print("ğŸ“ å¼€å§‹åˆ†ç»„GRPOè®­ç»ƒ")
        print("="*60)

        max_steps = self.config['max_steps']
        save_every = self.config.get('save_every', 50)
        log_every = self.config.get('log_every', 5)

        for step in range(1, max_steps + 1):
            try:
                metrics = await self.train_step(step)

                # ä¿å­˜æ£€æŸ¥ç‚¹
                if step % save_every == 0:
                    self._save_checkpoint(step)

            except Exception as e:
                print(f"\nâŒ Step {step} å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                continue

        print("\n" + "="*60)
        print("âœ… è®­ç»ƒå®Œæˆ")
        print("="*60)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_checkpoint(max_steps, final=True)
        wandb.finish()

    def _save_checkpoint(self, step: int, final: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        output_dir = Path(self.config['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)

        if final:
            save_path = output_dir / "final_model"
        else:
            save_path = output_dir / f"checkpoint-{step}"

        self.model.save_pretrained(save_path)
        self.tokenizer.save_pretrained(save_path)

        print(f"\nğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="åˆ†ç»„GRPOè®­ç»ƒ")
    parser.add_argument(
        '--config',
        type=str,
        default='config/grouped_training.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    args = parser.parse_args()

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                              â•‘
â•‘     åˆ†ç»„GRPOè®­ç»ƒ - å¤šé—®é¢˜åŠ æƒè¯„åˆ† + å¤šæ ·æ€§æ‰“ç ´å¹³å±€          â•‘
â•‘                                                              â•‘
â•‘     æ¯ step: 3 ç»„ Ã— 4 é—®é¢˜ = 12 é—®é¢˜                        â•‘
â•‘     ç¡®ä¿åŒ…å«: math + qa + code                               â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    trainer = GroupedGRPOTrainer(config_path=args.config)
    await trainer.train()


if __name__ == "__main__":
    asyncio.run(main())
