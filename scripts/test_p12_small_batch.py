#!/usr/bin/env python3
"""
P12å°è§„æ¨¡æµ‹è¯• - ç›´æ¥æµ‹è¯•LLMæå– (8002ç«¯å£OSSæ¨¡å‹)
"""

import sys
import os

# ç¦ç”¨ä»£ç†ï¼Œç¡®ä¿ç›´è¿localhost:8002
os.environ.pop('http_proxy', None)
os.environ.pop('https_proxy', None)
os.environ.pop('HTTP_PROXY', None)
os.environ.pop('HTTPS_PROXY', None)
os.environ['no_proxy'] = 'localhost,127.0.0.1'

os.environ['CUDA_VISIBLE_DEVICES'] = '3'
sys.path.insert(0, '/home/yijia/.claude/11/integrated_aflow_roll/src')

from reward_computer import RewardComputer


def main():
    print("\n" + "#" * 60)
    print("# P12å°è§„æ¨¡æµ‹è¯• - LLMæå–éªŒè¯ (8002ç«¯å£OSS)")
    print("#" * 60)

    # åˆå§‹åŒ–
    print("\n[1/2] åˆå§‹åŒ–RewardComputer...")
    rc = RewardComputer(use_llm_judge=True, debug_logging=True)

    if not rc.llm_judge_client:
        print("âŒ LLM Judgeå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
        return 1
    print(f"âœ… LLM Judgeå®¢æˆ·ç«¯å°±ç»ª: {rc.llm_judge_model}")

    # æµ‹è¯•æ¡ˆä¾‹ - æ¥è‡ªçœŸå®è®­ç»ƒæ—¥å¿—
    print("\n[2/2] æµ‹è¯•LLMæå–æ•ˆæœ...")

    test_cases = [
        # æ¡ˆä¾‹1: åŒ…å«Pythonä»£ç çš„æ•°å­¦è§£ç­” (ä¹‹å‰è¿”å›0.2)
        {
            'name': 'Math+Pythonä»£ç ',
            'problem': 'In a glee club, there are two times as many female than male members. How many female members are there if there are 18 members in the club?',
            'prediction': '''**Answer**
There are **12 female members**.

**Python code to compute the answer**

```python
total_members = 18
male_members = total_members // 3
female_members = 2 * male_members
print(female_members)  # Output: 12
```''',
            'ground_truth': '12',
            'problem_type': 'math',
            'source': 'gsm8k',
            'old_score': 0.2,
            'expected_score': 1.0,
        },
        # æ¡ˆä¾‹2: Step-by-stepè§£ç­”
        {
            'name': 'Math Stepè§£ç­”',
            'problem': 'Two identical CDs regularly cost a total of $28. What is the cost in dollars of five of these CDs?',
            'prediction': '''**Step 1 â€“ Identify what the problem is asking**
We need to find the cost of five CDs.

**Step 2 â€“ Calculate**
Two CDs cost $28, so one CD costs $14.
Five CDs cost 5 Ã— $14 = $70.

Therefore, the answer is **70**.

\\boxed{70}''',
            'ground_truth': '70',
            'problem_type': 'math',
            'source': 'math',
            'old_score': 0.2,
            'expected_score': 1.0,
        },
        # æ¡ˆä¾‹3: Thereforeç»“è®º
        {
            'name': 'Thereforeç»“è®º',
            'problem': 'What is 15 + 27?',
            'prediction': 'Let me calculate: 15 + 27 = 42. Therefore, the answer is 42.',
            'ground_truth': '42',
            'problem_type': 'math',
            'source': 'math',
            'old_score': 0.2,
            'expected_score': 1.0,
        },
        # æ¡ˆä¾‹4: QAè§£é‡Šæ€§å›ç­”
        {
            'name': 'QAè§£é‡Šæ€§å›ç­”',
            'problem': 'What is the capital of France?',
            'prediction': '''**Answer**

The capital of France is **Paris**.

**Explanation**
Paris has been the capital since the 10th century and is the political, economic, and cultural center of France.''',
            'ground_truth': 'Paris',
            'problem_type': 'qa',
            'source': 'hotpotqa',
            'old_score': 0.2,
            'expected_score': 1.0,
        },
    ]

    results = {'improved': 0, 'same': 0, 'worse': 0}

    for i, test in enumerate(test_cases):
        print(f"\n{'='*55}")
        print(f"æµ‹è¯• {i+1}/{len(test_cases)}: {test['name']}")
        print(f"{'='*55}")
        print(f"é—®é¢˜: {test['problem'][:60]}...")
        print(f"ç­”æ¡ˆ: {test['ground_truth']}")
        print(f"é¢„æµ‹: {test['prediction'][:80]}...")
        print(f"æ—§åˆ†æ•°: {test['old_score']} -> æœŸæœ›: {test['expected_score']}")
        print()

        try:
            if test['problem_type'] == 'math':
                reward = rc._compute_math_reward(
                    problem=test['problem'],
                    prediction=test['prediction'],
                    ground_truth=test['ground_truth'],
                    source=test['source']
                )
            else:
                reward = rc._compute_qa_reward(
                    problem=test['problem'],
                    prediction=test['prediction'],
                    ground_truth=test['ground_truth'],
                    source=test.get('source', 'hotpotqa')
                )

            print(f"\næ–°åˆ†æ•°: {reward}")

            if reward > test['old_score']:
                print(f"âœ… æ”¹è¿›! {test['old_score']} -> {reward}")
                results['improved'] += 1
            elif reward == test['old_score']:
                print(f"â¡ï¸  ç›¸åŒ: {reward}")
                results['same'] += 1
            else:
                print(f"âŒ å˜å·®: {test['old_score']} -> {reward}")
                results['worse'] += 1

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results['worse'] += 1

    # æ±‡æ€»
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ±‡æ€»")
    print("=" * 60)
    print(f"  âœ… æ”¹è¿›: {results['improved']}/{len(test_cases)}")
    print(f"  â¡ï¸  ç›¸åŒ: {results['same']}/{len(test_cases)}")
    print(f"  âŒ å˜å·®: {results['worse']}/{len(test_cases)}")

    # LLMç»Ÿè®¡
    print(f"\nLLM Judgeç»Ÿè®¡:")
    print(f"  æˆåŠŸè°ƒç”¨: {rc.eval_stats.get('llm_judge_success', 0)}")
    print(f"  APIå¤±è´¥: {rc.eval_stats.get('llm_judge_api_failures', 0)}")

    print("\n" + "=" * 60)
    if results['improved'] >= len(test_cases) * 0.75:  # 75%æ”¹è¿›
        print("ğŸ‰ P12ä¿®å¤æœ‰æ•ˆï¼å¤§éƒ¨åˆ†æ¡ˆä¾‹å¾—åˆ°æ”¹è¿›")
        print("   å»ºè®®å¯åŠ¨å®Œæ•´è®­ç»ƒéªŒè¯")
        return 0
    elif results['worse'] == 0:
        print("âœ… P12ä¿®å¤å®‰å…¨ï¼Œæ²¡æœ‰æ¡ˆä¾‹å˜å·®")
        return 0
    else:
        print("âš ï¸  éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
        return 1


if __name__ == "__main__":
    sys.exit(main())
