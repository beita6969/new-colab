# ğŸ“š Complete Dataset Setup Guide for AFlow+ROLL

## å¿«é€Ÿå¼€å§‹

### 1. ä¸‹è½½æ‰€æœ‰æ•°æ®é›†

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /home/yijia/.claude/11/integrated_aflow_roll

# å®‰è£…ä¾èµ–
pip install datasets transformers tqdm requests

# è¿è¡Œä¸‹è½½è„šæœ¬
python scripts/download_all_datasets.py
```

è¿™å°†ä¸‹è½½ä»¥ä¸‹æ•°æ®é›†ï¼š
- **GSM8K** (8,500é¢˜) - æ•°å­¦æ¨ç†
- **HumanEval** (164é¢˜) - ä»£ç ç”Ÿæˆ
- **MBPP** (1,000é¢˜) - åŸºç¡€ç¼–ç¨‹
- **CommonsenseQA** (12,247é¢˜) - å¸¸è¯†æ¨ç†
- **HotpotQA** (113,000é¢˜) - å¤šè·³æ¨ç†
- **MMLU** (15,000é¢˜) - å¤šé¢†åŸŸçŸ¥è¯†

### 2. æ•°æ®é›†ç»“æ„

ä¸‹è½½å®Œæˆåï¼Œæ•°æ®é›†å°†æŒ‰ä»¥ä¸‹ç»“æ„ç»„ç»‡ï¼š

```
data/
â”œâ”€â”€ raw/                    # åŸå§‹æ•°æ®é›†
â”‚   â”œâ”€â”€ gsm8k/
â”‚   â”‚   â”œâ”€â”€ train.jsonl    # 7,473 è®­ç»ƒæ ·æœ¬
â”‚   â”‚   â””â”€â”€ test.jsonl     # 1,319 æµ‹è¯•æ ·æœ¬
â”‚   â”œâ”€â”€ humaneval/
â”‚   â”‚   â””â”€â”€ HumanEval.jsonl # 164 ç¼–ç¨‹é¢˜
â”‚   â”œâ”€â”€ mbpp/
â”‚   â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”‚   â”œâ”€â”€ validation.jsonl
â”‚   â”‚   â””â”€â”€ test.jsonl
â”‚   â”œâ”€â”€ commonsenseqa/
â”‚   â”‚   â”œâ”€â”€ train.jsonl    # 9,741 è®­ç»ƒ
â”‚   â”‚   â”œâ”€â”€ validation.jsonl # 1,221 éªŒè¯
â”‚   â”‚   â””â”€â”€ test.jsonl     # 1,285 æµ‹è¯•
â”‚   â”œâ”€â”€ hotpotqa/
â”‚   â”‚   â””â”€â”€ dev_distractor.json
â”‚   â””â”€â”€ mmlu/
â”‚       â”œâ”€â”€ train.jsonl
â”‚       â”œâ”€â”€ validation.jsonl
â”‚       â””â”€â”€ test.jsonl
â”‚
â””â”€â”€ processed/              # å¤„ç†åçš„æ··åˆæ•°æ®
    â”œâ”€â”€ train_mixed.jsonl   # 1000 è®­ç»ƒæ ·æœ¬
    â”œâ”€â”€ val_mixed.jsonl     # 100 éªŒè¯æ ·æœ¬
    â””â”€â”€ test_mixed.jsonl    # 100 æµ‹è¯•æ ·æœ¬
```

## æ•°æ®é›†è¯¦æƒ…

### æ•°å­¦æ¨ç† - GSM8K

**æ ¼å¼ç¤ºä¾‹**ï¼š
```json
{
  "question": "Natalia sold clips to 48 of her friends...",
  "answer": "Natalia sold 48/2 = <<48/2=24>>24 clips...\n#### 72"
}
```

**è¯„ä¼°æ–¹æ³•**ï¼š
- æå–`####`åçš„æ•°å€¼ç­”æ¡ˆ
- æ•°å€¼å®¹å·®æ¯”è¾ƒ (1e-4)
- å‡†ç¡®ç‡è®¡ç®—

### ä»£ç ç”Ÿæˆ - HumanEval & MBPP

**HumanEvalæ ¼å¼**ï¼š
```json
{
  "task_id": "HumanEval/0",
  "prompt": "def has_close_elements(numbers: List[float], threshold: float) -> bool:",
  "entry_point": "has_close_elements",
  "test": "def check(candidate):\n    assert candidate(...)"
}
```

**è¯„ä¼°æ–¹æ³•**ï¼š
- Pass@kæŒ‡æ ‡ (k=1,10,100)
- ä»£ç æ‰§è¡Œæµ‹è¯•
- è¶…æ—¶ä¿æŠ¤ (5ç§’)

### é—®ç­” - CommonsenseQA

**æ ¼å¼ç¤ºä¾‹**ï¼š
```json
{
  "question": "Where do you put groceries?",
  "choices": {
    "label": ["A", "B", "C", "D", "E"],
    "text": ["pantry", "shelf", "refrigerator", "cabinet", "kitchen"]
  },
  "answerKey": "C"
}
```

**è¯„ä¼°æ–¹æ³•**ï¼š
- å¤šé€‰é¢˜å‡†ç¡®ç‡
- é€‰é¡¹åŒ¹é…

### å¤šè·³æ¨ç† - HotpotQA

**æ ¼å¼ç¤ºä¾‹**ï¼š
```json
{
  "question": "What government position was held by...",
  "answer": "Chief of Protocol",
  "supporting_facts": [["title1", 0], ["title2", 2]],
  "context": [["title", ["sentence1", "sentence2"]]]
}
```

**è¯„ä¼°æ–¹æ³•**ï¼š
- ç­”æ¡ˆF1åˆ†æ•°
- æ”¯æ’‘äº‹å®F1åˆ†æ•°
- è”åˆè¯„åˆ†

### ç»¼åˆè¯„ä¼° - MMLU

**æ ¼å¼ç¤ºä¾‹**ï¼š
```json
{
  "question": "Question text",
  "choices": ["A", "B", "C", "D"],
  "answer": "B",
  "subject": "abstract_algebra"
}
```

**è¯„ä¼°æ–¹æ³•**ï¼š
- 57ä¸ªå­¦ç§‘åˆ†ç±»å‡†ç¡®ç‡
- æ•´ä½“å‡†ç¡®ç‡
- é¢†åŸŸåˆ«å‡†ç¡®ç‡ (STEM/äººæ–‡/ç¤¾ç§‘)

## ä½¿ç”¨è¯„ä¼°å‡½æ•°

### åŸºç¡€ä½¿ç”¨

```python
from src.unified_evaluator import UnifiedEvaluator

# åˆ›å»ºè¯„ä¼°å™¨
evaluator = UnifiedEvaluator()

# è¯„ä¼°æ•°å­¦é¢˜
math_result = evaluator.evaluate(
    prediction="The answer is 42.",
    ground_truth="#### 42",
    problem_type="math"
)
print(f"Math correct: {math_result['correct']}")

# è¯„ä¼°ä»£ç 
code_result = evaluator.evaluate(
    prediction="def add(a, b): return a + b",
    ground_truth="def add(a, b): return a + b",
    problem_type="code",
    test="assert add(1, 2) == 3"
)
print(f"Code passed: {code_result['correct']}")

# è¯„ä¼°é—®ç­”
qa_result = evaluator.evaluate(
    prediction="The answer is B.",
    ground_truth="B",
    problem_type="multiple_choice"
)
print(f"QA correct: {qa_result['correct']}")
```

### æ‰¹é‡è¯„ä¼°

```python
from src.unified_evaluator import DatasetSpecificEvaluator

# åˆ›å»ºæ•°æ®é›†è¯„ä¼°å™¨
ds_evaluator = DatasetSpecificEvaluator()

# è¯„ä¼°GSM8K
gsm8k_results = ds_evaluator.evaluate_gsm8k(
    predictions=model_predictions,
    dataset_path="./data/raw/gsm8k/test.jsonl"
)
print(f"GSM8K Accuracy: {gsm8k_results['overall_accuracy']:.2%}")

# è¯„ä¼°HumanEval
humaneval_results = ds_evaluator.evaluate_humaneval(
    predictions=model_predictions,
    dataset_path="./data/raw/humaneval/HumanEval.jsonl",
    k_values=[1, 10, 100]
)
print(f"Pass@1: {humaneval_results['pass_at_k']['pass@1']:.2%}")

# è¯„ä¼°MMLU
mmlu_results = ds_evaluator.evaluate_mmlu(
    predictions=model_predictions,
    dataset_path="./data/raw/mmlu/test.jsonl"
)
print(f"MMLU Accuracy: {mmlu_results['overall_accuracy']:.2%}")
```

## é›†æˆåˆ°è®­ç»ƒæµç¨‹

### 1. æ›´æ–°è®­ç»ƒé…ç½®

ç¼–è¾‘ `config/training.yaml`ï¼Œæ·»åŠ æ•°æ®é›†é…ç½®ï¼š

```yaml
data:
  train_path: ./data/processed/train_mixed.jsonl
  val_path: ./data/processed/val_mixed.jsonl
  test_path: ./data/processed/test_mixed.jsonl

  # é¢†åŸŸæ¯”ä¾‹
  domain_ratios:
    math: 0.30    # GSM8K
    code: 0.25    # HumanEval + MBPP
    qa: 0.25      # CommonsenseQA + HotpotQA
    mixed: 0.20   # MMLU
```

### 2. è¿è¡Œè®­ç»ƒ

```bash
# ä½¿ç”¨æ–°æ•°æ®é›†è®­ç»ƒ
python train.py --config config/training.yaml
```

### 3. ç›‘æ§è®­ç»ƒ

```bash
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f logs/training.log

# æŸ¥çœ‹è¯„ä¼°ç»“æœ
python analyze_training.py --checkpoint checkpoints/step_50
```

## æ•°æ®é›†ç»Ÿè®¡

| æ•°æ®é›† | è®­ç»ƒ | éªŒè¯ | æµ‹è¯• | ç±»å‹ | è¯„ä¼°æŒ‡æ ‡ |
|--------|------|------|------|------|----------|
| GSM8K | 7,473 | - | 1,319 | æ•°å­¦ | å‡†ç¡®ç‡ |
| HumanEval | - | - | 164 | ä»£ç  | Pass@k |
| MBPP | ~374 | 90 | 500 | ä»£ç  | Pass@k |
| CommonsenseQA | 9,741 | 1,221 | 1,285 | QA | å‡†ç¡®ç‡ |
| HotpotQA | 90,000+ | 7,000+ | - | å¤šè·³ | F1åˆ†æ•° |
| MMLU | - | 285 | 14,042 | ç»¼åˆ | å‡†ç¡®ç‡ |

## æ€§èƒ½åŸºå‡†

é¢„æœŸæ€§èƒ½æŒ‡æ ‡ï¼ˆåŸºäºQwen2.5-7B + LoRAï¼‰ï¼š

- **GSM8K**: 60-70% å‡†ç¡®ç‡
- **HumanEval**: Pass@1 30-40%
- **MBPP**: Pass@1 40-50%
- **CommonsenseQA**: 70-75% å‡†ç¡®ç‡
- **HotpotQA**: F1 60-65%
- **MMLU**: 55-60% å‡†ç¡®ç‡

ç»è¿‡GRPOè®­ç»ƒåï¼Œé¢„æœŸæå‡10-15%ã€‚

## å¸¸è§é—®é¢˜

### Q: ä¸‹è½½å¤±è´¥æ€ä¹ˆåŠï¼Ÿ
A: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œä½¿ç”¨ä»£ç†æˆ–é•œåƒæºï¼š
```bash
export HF_ENDPOINT=https://hf-mirror.com
```

### Q: å†…å­˜ä¸è¶³ï¼Ÿ
A: ä½¿ç”¨æµå¼åŠ è½½ï¼š
```python
dataset = load_dataset("dataset_name", streaming=True)
```

### Q: å¦‚ä½•æ·»åŠ æ–°æ•°æ®é›†ï¼Ÿ
A: ç¼–è¾‘ `config/datasets.yaml` æ·»åŠ æ–°æ•°æ®é›†é…ç½®ï¼Œç„¶åæ›´æ–°ä¸‹è½½è„šæœ¬ã€‚

## ä¸‹ä¸€æ­¥

1. âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆ
2. âœ… è¯„ä¼°å‡½æ•°å°±ç»ª
3. ğŸ¯ å¼€å§‹è®­ç»ƒï¼š`python train.py`
4. ğŸ“Š ç›‘æ§è¿›åº¦ï¼šä½¿ç”¨WandBæˆ–TensorBoard
5. ğŸ”¬ è°ƒä¼˜è¶…å‚æ•°ï¼šåŸºäºéªŒè¯é›†æ€§èƒ½

---

**æç¤º**ï¼šå»ºè®®å…ˆç”¨å°æ‰¹é‡æ•°æ®ï¼ˆ100-1000æ ·æœ¬ï¼‰æµ‹è¯•æµç¨‹ï¼Œç¡®è®¤æ— è¯¯åå†å…¨é‡è®­ç»ƒã€‚
